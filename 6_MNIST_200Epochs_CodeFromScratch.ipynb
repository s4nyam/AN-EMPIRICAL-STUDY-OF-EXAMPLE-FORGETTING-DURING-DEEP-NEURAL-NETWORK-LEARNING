{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_MNIST_200Epochs_CodeFromScratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHsPHrDk-Ee0",
        "outputId": "268ae763-19a4-46ff-8431-9ccd65bc1663"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V35ySJYcCFoc",
        "outputId": "e3e03d68-854d-4952-d30e-e738651a0813"
      },
      "source": [
        "!pwd\n",
        "!ls\n",
        "%cd drive\n",
        "%cd MyDrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "drive  sample_data\n",
            "/content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAVhVb0i-UKU",
        "outputId": "af2c4d9f-7146-4639-89ac-9f3e8979674e"
      },
      "source": [
        "!git clone https://github.com/mtoneva/example_forgetting.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'example_forgetting'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Total 194 (delta 0), reused 0 (delta 0), pack-reused 194\u001b[K\n",
            "Receiving objects: 100% (194/194), 566.18 KiB | 2.06 MiB/s, done.\n",
            "Resolving deltas: 100% (102/102), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyCLqv4J-tXf",
        "outputId": "207ceb65-e345-4d27-8863-0bd8c0045b41"
      },
      "source": [
        "%cd example_forgetting/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/example_forgetting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2swMilJ2-wXq",
        "outputId": "b7557b86-12fb-4018-c97e-a5af2dd8358a"
      },
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
            "Collecting torch==0.4.1.post2\n",
            "  Downloading torch-0.4.1.post2-cp37-cp37m-manylinux1_x86_64.whl (519.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 519.5 MB 23 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.1.8\n",
            "  Downloading torchvision-0.1.8-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from torchvision==0.1.8->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.1.8->-r requirements.txt (line 3)) (1.15.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 0.4.1.post2 which is incompatible.\n",
            "fastai 1.0.61 requires torch>=1.0.0, but you have torch 0.4.1.post2 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-0.4.1.post2 torchvision-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGT9kP5V-0g8"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMCW3TdJC1Sq"
      },
      "source": [
        "# Format time for printing purposes\n",
        "def get_hms(seconds):\n",
        "    m, s = divmod(seconds, 60)\n",
        "    h, m = divmod(m, 60)\n",
        "\n",
        "    return h, m, s\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZdO8pmDC23Z"
      },
      "source": [
        "# Setup basic CNN model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "\n",
        "        if args['no_dropout']:\n",
        "            x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        else:\n",
        "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        if not args['no_dropout']:\n",
        "            x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnHCxXY3C4iK"
      },
      "source": [
        "# Train model for one epoch\n",
        "#\n",
        "# example_stats: dictionary containing statistics accumulated over every presentation of example\n",
        "#\n",
        "def train(args, model, device, trainset, optimizer, epoch, example_stats):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    batch_size = args['batch_size']\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Get permutation to shuffle trainset\n",
        "    trainset_permutation_inds = npr.permutation(\n",
        "        np.arange(len(trainset.train_labels)))\n",
        "\n",
        "    for batch_idx, batch_start_ind in enumerate(\n",
        "            range(0, len(trainset.train_labels), batch_size)):\n",
        "\n",
        "        # Get trainset indices for batch\n",
        "        batch_inds = trainset_permutation_inds[batch_start_ind:\n",
        "                                               batch_start_ind + batch_size]\n",
        "\n",
        "        # Get batch inputs and targets, transform them appropriately\n",
        "        transformed_trainset = []\n",
        "        for ind in batch_inds:\n",
        "            transformed_trainset.append(trainset.__getitem__(ind)[0])\n",
        "        inputs = torch.stack(transformed_trainset)\n",
        "        targets = torch.LongTensor(\n",
        "            np.array(trainset.train_labels)[batch_inds].tolist())\n",
        "\n",
        "        # Map to available device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward propagation, compute loss, get predictions\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update statistics and loss\n",
        "        acc = predicted == targets\n",
        "        for j, index in enumerate(batch_inds):\n",
        "\n",
        "            # Get index in original dataset (not sorted by forgetting)\n",
        "            index_in_original_dataset = train_indx[index]\n",
        "\n",
        "            # Compute missclassification margin\n",
        "            output_correct_class = outputs.data[\n",
        "                j, targets[j].item()]  # output for correct class\n",
        "            sorted_output, _ = torch.sort(outputs.data[j, :])\n",
        "            if acc[j]:\n",
        "                # Example classified correctly, highest incorrect class is 2nd largest output\n",
        "                output_highest_incorrect_class = sorted_output[-2]\n",
        "            else:\n",
        "                # Example misclassified, highest incorrect class is max output\n",
        "                output_highest_incorrect_class = sorted_output[-1]\n",
        "            margin = output_correct_class.item(\n",
        "            ) - output_highest_incorrect_class.item()\n",
        "\n",
        "            # Add the statistics of the current training example to dictionary\n",
        "            index_stats = example_stats.get(index_in_original_dataset,\n",
        "                                            [[], [], []])\n",
        "            index_stats[0].append(loss[j].item())\n",
        "            index_stats[1].append(acc[j].sum().item())\n",
        "            index_stats[2].append(margin)\n",
        "            example_stats[index_in_original_dataset] = index_stats\n",
        "\n",
        "        # Update loss, backward propagate, update optimizer\n",
        "        loss = loss.mean()\n",
        "        train_loss += loss.item()\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write(\n",
        "            '| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%' %\n",
        "            (epoch, args['epochs'], batch_idx + 1,\n",
        "             (len(trainset) // batch_size) + 1, loss.item(),\n",
        "             100. * correct.item() / total))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Add training accuracy to dict\n",
        "        index_stats = example_stats.get('train', [[], []])\n",
        "        index_stats[1].append(100. * correct.item() / float(total))\n",
        "        example_stats['train'] = index_stats\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlQdyfzjDPaS"
      },
      "source": [
        "# Evaluate model predictions on heldout test data\n",
        "#\n",
        "# example_stats: dictionary containing statistics accumulated over every presentation of example\n",
        "#\n",
        "def test(args, model, device, testset, example_stats):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_batch_size = 32\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch_idx, batch_start_ind in enumerate(\n",
        "            range(0, len(testset.test_labels), test_batch_size)):\n",
        "\n",
        "        # Get batch inputs and targets\n",
        "        transformed_testset = []\n",
        "        for ind in range(\n",
        "                batch_start_ind,\n",
        "                min(\n",
        "                    len(testset.test_labels),\n",
        "                    batch_start_ind + test_batch_size)):\n",
        "            transformed_testset.append(testset.__getitem__(ind)[0])\n",
        "        inputs = torch.stack(transformed_testset)\n",
        "        targets = torch.LongTensor(\n",
        "            np.array(testset.test_labels)[batch_start_ind:batch_start_ind +\n",
        "                                          test_batch_size].tolist())\n",
        "\n",
        "        # Map to available device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward propagation, compute loss, get predictions\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss = loss.mean()\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "    # Add test accuracy to dict\n",
        "    acc = 100. * correct.item() / total\n",
        "    index_stats = example_stats.get('test', [[], []])\n",
        "    index_stats[1].append(100. * correct.item() / float(total))\n",
        "    example_stats['test'] = index_stats\n",
        "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %\n",
        "          (epoch, loss.item(), acc))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptLDUWR0DTdq",
        "outputId": "7b3df71e-7819-4c1c-c11c-2cd76a85cd9d"
      },
      "source": [
        "args = {'dataset': 'mnist',\n",
        "        'batch_size': 64,\n",
        "        'epochs':200,\n",
        "        'lr':0.01,\n",
        "        'momentum':0.5,\n",
        "        'no_cuda':False,\n",
        "        'seed':2,\n",
        "        'sorting_file':\"none\",\n",
        "        'remove_n':0,\n",
        "        'keep_lowest_n':0,\n",
        "        'no_dropout':False,\n",
        "        'input_dir':'mnist_results/',\n",
        "        'output_dir':'mnist_results/'\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "# Set appropriate devices\n",
        "use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "# Set random seed for initialization\n",
        "torch.manual_seed(args['seed'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(args['seed'])\n",
        "npr.seed(args['seed'])\n",
        "\n",
        "# Setup transforms\n",
        "all_transforms = [\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307, ), (0.3081, ))\n",
        "]\n",
        "if args['dataset'] == 'permuted_mnist':\n",
        "    pixel_permutation = torch.randperm(28 * 28)\n",
        "    all_transforms.append(\n",
        "        transforms.Lambda(\n",
        "            lambda x: x.view(-1, 1)[pixel_permutation].view(1, 28, 28)))\n",
        "transform = transforms.Compose(all_transforms)\n",
        "\n",
        "os.makedirs(args['output_dir'], exist_ok=True)\n",
        "\n",
        "# Load the appropriate train and test datasets\n",
        "trainset = datasets.MNIST(\n",
        "    root='/tmp/data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(\n",
        "    root='/tmp/data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Get indices of examples that should be used for training\n",
        "if args['sorting_file'] == 'none':\n",
        "    train_indx = np.array(range(len(trainset.train_labels)))\n",
        "else:\n",
        "    try:\n",
        "        with open(\n",
        "                os.path.join(args['input_dir'], args['sorting_file']) + '.pkl',\n",
        "                'rb') as fin:\n",
        "            ordered_indx = pickle.load(fin)['indices']\n",
        "    except IOError:\n",
        "        with open(os.path.join(args['input_dir'], args['sorting_file']),\n",
        "                  'rb') as fin:\n",
        "            ordered_indx = pickle.load(fin)['indices']\n",
        "\n",
        "    # Get the indices to remove from training\n",
        "    elements_to_remove = np.array(\n",
        "        ordered_indx)[args['keep_lowest_n']:args['keep_lowest_n'] + args['remove_n']]\n",
        "\n",
        "    # Remove the corresponding elements\n",
        "    train_indx = np.setdiff1d(\n",
        "        range(len(trainset.train_labels)), elements_to_remove)\n",
        "\n",
        "# Remove remove_n number of examples from the train set at random\n",
        "if args['keep_lowest_n'] < 0:\n",
        "    train_indx = npr.permutation(np.arange(len(\n",
        "        trainset.train_labels)))[:len(trainset.train_labels) - args['remove_n']]\n",
        "\n",
        "# Reassign train data and labels\n",
        "trainset.train_data = trainset.train_data[train_indx, :, :]\n",
        "trainset.train_labels = np.array(trainset.train_labels)[train_indx].tolist()\n",
        "\n",
        "print('Training on ' + str(len(trainset.train_labels)) + ' examples')\n",
        "\n",
        "# Setup model and optimizer\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "# Setup loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.__init__(reduce=False)\n",
        "\n",
        "# Initialize dictionary to save statistics for every example presentation\n",
        "example_stats = {}\n",
        "\n",
        "elapsed_time = 0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Training on 60000 examples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyS_EDlHEZYb",
        "outputId": "f00c8bdb-f94d-4dcf-a07c-525f607c4982"
      },
      "source": [
        "for epoch in range(args['epochs']):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train(args, model, device, trainset, optimizer, epoch, example_stats)\n",
        "    test(args, model, device, testset, example_stats)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    elapsed_time += epoch_time\n",
        "    print('| Elapsed time : %d:%02d:%02d' % (get_hms(elapsed_time)))\n",
        "\n",
        "    # Save the stats dictionary\n",
        "    fname = os.path.join(args['output_dir'], str(epoch))\n",
        "    with open(fname + \"__stats_dict.pkl\", \"wb\") as f:\n",
        "        pickle.dump(example_stats, f)\n",
        "\n",
        "    # Log the best train and test accuracy so far\n",
        "    with open(fname + \"__best_acc.txt\", \"w\") as f:\n",
        "        f.write('train test \\n')\n",
        "        f.write(str(max(example_stats['train'][1])))\n",
        "        f.write(' ')\n",
        "        f.write(str(max(example_stats['test'][1])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch [  0/200] Iter[938/938]\t\tLoss: 0.3230 Acc@1: 68.837%\n",
            "| Validation Epoch #0\t\t\tLoss: 0.0504 Acc@1: 94.35%\n",
            "| Elapsed time : 0:00:38\n",
            "| Epoch [  1/200] Iter[938/938]\t\tLoss: 0.1633 Acc@1: 88.528%\n",
            "| Validation Epoch #1\t\t\tLoss: 0.0139 Acc@1: 96.40%\n",
            "| Elapsed time : 0:01:17\n",
            "| Epoch [  2/200] Iter[938/938]\t\tLoss: 0.0713 Acc@1: 91.125%\n",
            "| Validation Epoch #2\t\t\tLoss: 0.0042 Acc@1: 96.95%\n",
            "| Elapsed time : 0:01:55\n",
            "| Epoch [  3/200] Iter[938/938]\t\tLoss: 0.1274 Acc@1: 92.275%\n",
            "| Validation Epoch #3\t\t\tLoss: 0.0021 Acc@1: 97.65%\n",
            "| Elapsed time : 0:02:34\n",
            "| Epoch [  4/200] Iter[938/938]\t\tLoss: 0.4450 Acc@1: 93.190%\n",
            "| Validation Epoch #4\t\t\tLoss: 0.0021 Acc@1: 97.73%\n",
            "| Elapsed time : 0:03:13\n",
            "| Epoch [  5/200] Iter[938/938]\t\tLoss: 0.0409 Acc@1: 93.782%\n",
            "| Validation Epoch #5\t\t\tLoss: 0.0006 Acc@1: 97.99%\n",
            "| Elapsed time : 0:03:51\n",
            "| Epoch [  6/200] Iter[938/938]\t\tLoss: 0.5165 Acc@1: 94.090%\n",
            "| Validation Epoch #6\t\t\tLoss: 0.0004 Acc@1: 98.06%\n",
            "| Elapsed time : 0:04:30\n",
            "| Epoch [  7/200] Iter[938/938]\t\tLoss: 0.1307 Acc@1: 94.535%\n",
            "| Validation Epoch #7\t\t\tLoss: 0.0004 Acc@1: 98.20%\n",
            "| Elapsed time : 0:05:09\n",
            "| Epoch [  8/200] Iter[938/938]\t\tLoss: 0.3734 Acc@1: 94.748%\n",
            "| Validation Epoch #8\t\t\tLoss: 0.0003 Acc@1: 98.20%\n",
            "| Elapsed time : 0:05:47\n",
            "| Epoch [  9/200] Iter[938/938]\t\tLoss: 0.0484 Acc@1: 94.932%\n",
            "| Validation Epoch #9\t\t\tLoss: 0.0002 Acc@1: 98.20%\n",
            "| Elapsed time : 0:06:26\n",
            "| Epoch [ 10/200] Iter[938/938]\t\tLoss: 0.0764 Acc@1: 95.090%\n",
            "| Validation Epoch #10\t\t\tLoss: 0.0004 Acc@1: 98.50%\n",
            "| Elapsed time : 0:07:05\n",
            "| Epoch [ 11/200] Iter[938/938]\t\tLoss: 0.3101 Acc@1: 95.222%\n",
            "| Validation Epoch #11\t\t\tLoss: 0.0001 Acc@1: 98.41%\n",
            "| Elapsed time : 0:07:44\n",
            "| Epoch [ 12/200] Iter[938/938]\t\tLoss: 0.4274 Acc@1: 95.382%\n",
            "| Validation Epoch #12\t\t\tLoss: 0.0003 Acc@1: 98.45%\n",
            "| Elapsed time : 0:08:23\n",
            "| Epoch [ 13/200] Iter[938/938]\t\tLoss: 0.1083 Acc@1: 95.492%\n",
            "| Validation Epoch #13\t\t\tLoss: 0.0001 Acc@1: 98.57%\n",
            "| Elapsed time : 0:09:02\n",
            "| Epoch [ 14/200] Iter[938/938]\t\tLoss: 0.1461 Acc@1: 95.610%\n",
            "| Validation Epoch #14\t\t\tLoss: 0.0001 Acc@1: 98.58%\n",
            "| Elapsed time : 0:09:40\n",
            "| Epoch [ 15/200] Iter[938/938]\t\tLoss: 0.1794 Acc@1: 95.672%\n",
            "| Validation Epoch #15\t\t\tLoss: 0.0001 Acc@1: 98.44%\n",
            "| Elapsed time : 0:10:19\n",
            "| Epoch [ 16/200] Iter[938/938]\t\tLoss: 0.2431 Acc@1: 95.927%\n",
            "| Validation Epoch #16\t\t\tLoss: 0.0003 Acc@1: 98.67%\n",
            "| Elapsed time : 0:10:58\n",
            "| Epoch [ 17/200] Iter[938/938]\t\tLoss: 0.1229 Acc@1: 96.010%\n",
            "| Validation Epoch #17\t\t\tLoss: 0.0001 Acc@1: 98.69%\n",
            "| Elapsed time : 0:11:37\n",
            "| Epoch [ 18/200] Iter[938/938]\t\tLoss: 0.1751 Acc@1: 95.867%\n",
            "| Validation Epoch #18\t\t\tLoss: 0.0001 Acc@1: 98.82%\n",
            "| Elapsed time : 0:12:16\n",
            "| Epoch [ 19/200] Iter[938/938]\t\tLoss: 0.1541 Acc@1: 96.090%\n",
            "| Validation Epoch #19\t\t\tLoss: 0.0001 Acc@1: 98.84%\n",
            "| Elapsed time : 0:12:55\n",
            "| Epoch [ 20/200] Iter[938/938]\t\tLoss: 0.1040 Acc@1: 96.135%\n",
            "| Validation Epoch #20\t\t\tLoss: 0.0001 Acc@1: 98.80%\n",
            "| Elapsed time : 0:13:34\n",
            "| Epoch [ 21/200] Iter[938/938]\t\tLoss: 0.1447 Acc@1: 96.140%\n",
            "| Validation Epoch #21\t\t\tLoss: 0.0001 Acc@1: 98.81%\n",
            "| Elapsed time : 0:14:13\n",
            "| Epoch [ 22/200] Iter[938/938]\t\tLoss: 0.2676 Acc@1: 96.160%\n",
            "| Validation Epoch #22\t\t\tLoss: 0.0001 Acc@1: 98.84%\n",
            "| Elapsed time : 0:14:53\n",
            "| Epoch [ 23/200] Iter[938/938]\t\tLoss: 0.0576 Acc@1: 96.248%\n",
            "| Validation Epoch #23\t\t\tLoss: 0.0001 Acc@1: 98.87%\n",
            "| Elapsed time : 0:15:31\n",
            "| Epoch [ 24/200] Iter[938/938]\t\tLoss: 0.0881 Acc@1: 96.342%\n",
            "| Validation Epoch #24\t\t\tLoss: 0.0001 Acc@1: 98.92%\n",
            "| Elapsed time : 0:16:10\n",
            "| Epoch [ 25/200] Iter[938/938]\t\tLoss: 0.1007 Acc@1: 96.352%\n",
            "| Validation Epoch #25\t\t\tLoss: 0.0001 Acc@1: 98.91%\n",
            "| Elapsed time : 0:16:50\n",
            "| Epoch [ 26/200] Iter[938/938]\t\tLoss: 0.0363 Acc@1: 96.468%\n",
            "| Validation Epoch #26\t\t\tLoss: 0.0001 Acc@1: 98.83%\n",
            "| Elapsed time : 0:17:29\n",
            "| Epoch [ 27/200] Iter[938/938]\t\tLoss: 0.2272 Acc@1: 96.342%\n",
            "| Validation Epoch #27\t\t\tLoss: 0.0005 Acc@1: 98.92%\n",
            "| Elapsed time : 0:18:08\n",
            "| Epoch [ 28/200] Iter[938/938]\t\tLoss: 0.0692 Acc@1: 96.497%\n",
            "| Validation Epoch #28\t\t\tLoss: 0.0000 Acc@1: 98.78%\n",
            "| Elapsed time : 0:18:48\n",
            "| Epoch [ 29/200] Iter[938/938]\t\tLoss: 0.0306 Acc@1: 96.633%\n",
            "| Validation Epoch #29\t\t\tLoss: 0.0001 Acc@1: 98.96%\n",
            "| Elapsed time : 0:19:27\n",
            "| Epoch [ 30/200] Iter[938/938]\t\tLoss: 0.1527 Acc@1: 96.603%\n",
            "| Validation Epoch #30\t\t\tLoss: 0.0001 Acc@1: 99.00%\n",
            "| Elapsed time : 0:20:06\n",
            "| Epoch [ 31/200] Iter[938/938]\t\tLoss: 0.2304 Acc@1: 96.587%\n",
            "| Validation Epoch #31\t\t\tLoss: 0.0002 Acc@1: 98.98%\n",
            "| Elapsed time : 0:20:46\n",
            "| Epoch [ 32/200] Iter[938/938]\t\tLoss: 0.2443 Acc@1: 96.572%\n",
            "| Validation Epoch #32\t\t\tLoss: 0.0002 Acc@1: 98.99%\n",
            "| Elapsed time : 0:21:25\n",
            "| Epoch [ 33/200] Iter[938/938]\t\tLoss: 0.1042 Acc@1: 96.573%\n",
            "| Validation Epoch #33\t\t\tLoss: 0.0001 Acc@1: 98.98%\n",
            "| Elapsed time : 0:22:05\n",
            "| Epoch [ 34/200] Iter[938/938]\t\tLoss: 0.0547 Acc@1: 96.672%\n",
            "| Validation Epoch #34\t\t\tLoss: 0.0001 Acc@1: 98.94%\n",
            "| Elapsed time : 0:22:44\n",
            "| Epoch [ 35/200] Iter[938/938]\t\tLoss: 0.1260 Acc@1: 96.700%\n",
            "| Validation Epoch #35\t\t\tLoss: 0.0000 Acc@1: 99.04%\n",
            "| Elapsed time : 0:23:23\n",
            "| Epoch [ 36/200] Iter[938/938]\t\tLoss: 0.1116 Acc@1: 96.723%\n",
            "| Validation Epoch #36\t\t\tLoss: 0.0001 Acc@1: 99.02%\n",
            "| Elapsed time : 0:24:02\n",
            "| Epoch [ 37/200] Iter[938/938]\t\tLoss: 0.0543 Acc@1: 96.725%\n",
            "| Validation Epoch #37\t\t\tLoss: 0.0000 Acc@1: 98.88%\n",
            "| Elapsed time : 0:24:41\n",
            "| Epoch [ 38/200] Iter[938/938]\t\tLoss: 0.0347 Acc@1: 96.748%\n",
            "| Validation Epoch #38\t\t\tLoss: 0.0000 Acc@1: 99.02%\n",
            "| Elapsed time : 0:25:22\n",
            "| Epoch [ 39/200] Iter[938/938]\t\tLoss: 0.0743 Acc@1: 96.828%\n",
            "| Validation Epoch #39\t\t\tLoss: 0.0001 Acc@1: 98.97%\n",
            "| Elapsed time : 0:26:01\n",
            "| Epoch [ 40/200] Iter[938/938]\t\tLoss: 0.0086 Acc@1: 96.722%\n",
            "| Validation Epoch #40\t\t\tLoss: 0.0009 Acc@1: 98.91%\n",
            "| Elapsed time : 0:26:40\n",
            "| Epoch [ 41/200] Iter[938/938]\t\tLoss: 0.1024 Acc@1: 96.897%\n",
            "| Validation Epoch #41\t\t\tLoss: 0.0001 Acc@1: 98.98%\n",
            "| Elapsed time : 0:27:19\n",
            "| Epoch [ 42/200] Iter[938/938]\t\tLoss: 0.0331 Acc@1: 96.938%\n",
            "| Validation Epoch #42\t\t\tLoss: 0.0000 Acc@1: 99.01%\n",
            "| Elapsed time : 0:27:58\n",
            "| Epoch [ 43/200] Iter[938/938]\t\tLoss: 0.1052 Acc@1: 96.862%\n",
            "| Validation Epoch #43\t\t\tLoss: 0.0001 Acc@1: 98.94%\n",
            "| Elapsed time : 0:28:37\n",
            "| Epoch [ 44/200] Iter[938/938]\t\tLoss: 0.0061 Acc@1: 96.848%\n",
            "| Validation Epoch #44\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 0:29:16\n",
            "| Epoch [ 45/200] Iter[938/938]\t\tLoss: 0.0286 Acc@1: 96.960%\n",
            "| Validation Epoch #45\t\t\tLoss: 0.0001 Acc@1: 98.90%\n",
            "| Elapsed time : 0:29:55\n",
            "| Epoch [ 46/200] Iter[938/938]\t\tLoss: 0.1667 Acc@1: 96.893%\n",
            "| Validation Epoch #46\t\t\tLoss: 0.0000 Acc@1: 98.99%\n",
            "| Elapsed time : 0:30:35\n",
            "| Epoch [ 47/200] Iter[938/938]\t\tLoss: 0.0728 Acc@1: 97.003%\n",
            "| Validation Epoch #47\t\t\tLoss: 0.0000 Acc@1: 99.04%\n",
            "| Elapsed time : 0:31:14\n",
            "| Epoch [ 48/200] Iter[938/938]\t\tLoss: 0.2207 Acc@1: 96.998%\n",
            "| Validation Epoch #48\t\t\tLoss: 0.0000 Acc@1: 99.10%\n",
            "| Elapsed time : 0:31:53\n",
            "| Epoch [ 49/200] Iter[938/938]\t\tLoss: 0.3133 Acc@1: 97.055%\n",
            "| Validation Epoch #49\t\t\tLoss: 0.0001 Acc@1: 98.99%\n",
            "| Elapsed time : 0:32:33\n",
            "| Epoch [ 50/200] Iter[938/938]\t\tLoss: 0.0352 Acc@1: 97.030%\n",
            "| Validation Epoch #50\t\t\tLoss: 0.0000 Acc@1: 99.06%\n",
            "| Elapsed time : 0:33:12\n",
            "| Epoch [ 51/200] Iter[938/938]\t\tLoss: 0.2024 Acc@1: 96.992%\n",
            "| Validation Epoch #51\t\t\tLoss: 0.0000 Acc@1: 98.99%\n",
            "| Elapsed time : 0:33:51\n",
            "| Epoch [ 52/200] Iter[938/938]\t\tLoss: 0.1708 Acc@1: 96.960%\n",
            "| Validation Epoch #52\t\t\tLoss: 0.0001 Acc@1: 99.05%\n",
            "| Elapsed time : 0:34:31\n",
            "| Epoch [ 53/200] Iter[938/938]\t\tLoss: 0.0678 Acc@1: 96.973%\n",
            "| Validation Epoch #53\t\t\tLoss: 0.0000 Acc@1: 99.01%\n",
            "| Elapsed time : 0:35:10\n",
            "| Epoch [ 54/200] Iter[938/938]\t\tLoss: 0.2380 Acc@1: 96.980%\n",
            "| Validation Epoch #54\t\t\tLoss: 0.0000 Acc@1: 98.98%\n",
            "| Elapsed time : 0:35:49\n",
            "| Epoch [ 55/200] Iter[938/938]\t\tLoss: 0.1447 Acc@1: 97.007%\n",
            "| Validation Epoch #55\t\t\tLoss: 0.0000 Acc@1: 99.03%\n",
            "| Elapsed time : 0:36:28\n",
            "| Epoch [ 56/200] Iter[938/938]\t\tLoss: 0.0391 Acc@1: 97.135%\n",
            "| Validation Epoch #56\t\t\tLoss: 0.0000 Acc@1: 99.04%\n",
            "| Elapsed time : 0:37:07\n",
            "| Epoch [ 57/200] Iter[938/938]\t\tLoss: 0.0715 Acc@1: 97.063%\n",
            "| Validation Epoch #57\t\t\tLoss: 0.0000 Acc@1: 99.03%\n",
            "| Elapsed time : 0:37:47\n",
            "| Epoch [ 58/200] Iter[938/938]\t\tLoss: 0.2810 Acc@1: 97.075%\n",
            "| Validation Epoch #58\t\t\tLoss: 0.0000 Acc@1: 99.05%\n",
            "| Elapsed time : 0:38:28\n",
            "| Epoch [ 59/200] Iter[938/938]\t\tLoss: 0.0873 Acc@1: 97.203%\n",
            "| Validation Epoch #59\t\t\tLoss: 0.0000 Acc@1: 99.08%\n",
            "| Elapsed time : 0:39:07\n",
            "| Epoch [ 60/200] Iter[938/938]\t\tLoss: 0.0913 Acc@1: 97.167%\n",
            "| Validation Epoch #60\t\t\tLoss: 0.0001 Acc@1: 99.13%\n",
            "| Elapsed time : 0:39:46\n",
            "| Epoch [ 61/200] Iter[938/938]\t\tLoss: 0.0217 Acc@1: 97.113%\n",
            "| Validation Epoch #61\t\t\tLoss: 0.0000 Acc@1: 99.05%\n",
            "| Elapsed time : 0:40:26\n",
            "| Epoch [ 62/200] Iter[938/938]\t\tLoss: 0.0500 Acc@1: 97.120%\n",
            "| Validation Epoch #62\t\t\tLoss: 0.0000 Acc@1: 99.05%\n",
            "| Elapsed time : 0:41:06\n",
            "| Epoch [ 63/200] Iter[938/938]\t\tLoss: 0.0117 Acc@1: 97.005%\n",
            "| Validation Epoch #63\t\t\tLoss: 0.0000 Acc@1: 99.00%\n",
            "| Elapsed time : 0:41:45\n",
            "| Epoch [ 64/200] Iter[938/938]\t\tLoss: 0.0554 Acc@1: 97.223%\n",
            "| Validation Epoch #64\t\t\tLoss: 0.0000 Acc@1: 98.98%\n",
            "| Elapsed time : 0:42:25\n",
            "| Epoch [ 65/200] Iter[938/938]\t\tLoss: 0.2426 Acc@1: 97.107%\n",
            "| Validation Epoch #65\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 0:43:05\n",
            "| Epoch [ 66/200] Iter[938/938]\t\tLoss: 0.0181 Acc@1: 97.207%\n",
            "| Validation Epoch #66\t\t\tLoss: 0.0001 Acc@1: 98.98%\n",
            "| Elapsed time : 0:43:45\n",
            "| Epoch [ 67/200] Iter[938/938]\t\tLoss: 0.0054 Acc@1: 97.152%\n",
            "| Validation Epoch #67\t\t\tLoss: 0.0001 Acc@1: 99.03%\n",
            "| Elapsed time : 0:44:24\n",
            "| Epoch [ 68/200] Iter[938/938]\t\tLoss: 0.0171 Acc@1: 97.210%\n",
            "| Validation Epoch #68\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 0:45:04\n",
            "| Epoch [ 69/200] Iter[938/938]\t\tLoss: 0.1787 Acc@1: 97.225%\n",
            "| Validation Epoch #69\t\t\tLoss: 0.0000 Acc@1: 99.04%\n",
            "| Elapsed time : 0:45:44\n",
            "| Epoch [ 70/200] Iter[938/938]\t\tLoss: 0.0407 Acc@1: 97.102%\n",
            "| Validation Epoch #70\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 0:46:24\n",
            "| Epoch [ 71/200] Iter[938/938]\t\tLoss: 0.0726 Acc@1: 97.332%\n",
            "| Validation Epoch #71\t\t\tLoss: 0.0000 Acc@1: 98.98%\n",
            "| Elapsed time : 0:47:03\n",
            "| Epoch [ 72/200] Iter[938/938]\t\tLoss: 0.1768 Acc@1: 97.373%\n",
            "| Validation Epoch #72\t\t\tLoss: 0.0000 Acc@1: 99.06%\n",
            "| Elapsed time : 0:47:45\n",
            "| Epoch [ 73/200] Iter[938/938]\t\tLoss: 0.0229 Acc@1: 97.213%\n",
            "| Validation Epoch #73\t\t\tLoss: 0.0000 Acc@1: 99.01%\n",
            "| Elapsed time : 0:48:24\n",
            "| Epoch [ 74/200] Iter[938/938]\t\tLoss: 0.0022 Acc@1: 97.245%\n",
            "| Validation Epoch #74\t\t\tLoss: 0.0000 Acc@1: 99.02%\n",
            "| Elapsed time : 0:49:04\n",
            "| Epoch [ 75/200] Iter[938/938]\t\tLoss: 0.0990 Acc@1: 97.273%\n",
            "| Validation Epoch #75\t\t\tLoss: 0.0001 Acc@1: 99.03%\n",
            "| Elapsed time : 0:49:44\n",
            "| Epoch [ 76/200] Iter[938/938]\t\tLoss: 0.1002 Acc@1: 97.225%\n",
            "| Validation Epoch #76\t\t\tLoss: 0.0000 Acc@1: 98.96%\n",
            "| Elapsed time : 0:50:24\n",
            "| Epoch [ 77/200] Iter[938/938]\t\tLoss: 0.1718 Acc@1: 97.168%\n",
            "| Validation Epoch #77\t\t\tLoss: 0.0000 Acc@1: 99.09%\n",
            "| Elapsed time : 0:51:03\n",
            "| Epoch [ 78/200] Iter[938/938]\t\tLoss: 0.0759 Acc@1: 97.278%\n",
            "| Validation Epoch #78\t\t\tLoss: 0.0000 Acc@1: 99.03%\n",
            "| Elapsed time : 0:51:43\n",
            "| Epoch [ 79/200] Iter[938/938]\t\tLoss: 0.2188 Acc@1: 97.200%\n",
            "| Validation Epoch #79\t\t\tLoss: 0.0000 Acc@1: 99.04%\n",
            "| Elapsed time : 0:52:23\n",
            "| Epoch [ 80/200] Iter[938/938]\t\tLoss: 0.2059 Acc@1: 97.367%\n",
            "| Validation Epoch #80\t\t\tLoss: 0.0000 Acc@1: 98.94%\n",
            "| Elapsed time : 0:53:03\n",
            "| Epoch [ 81/200] Iter[938/938]\t\tLoss: 0.3570 Acc@1: 97.347%\n",
            "| Validation Epoch #81\t\t\tLoss: 0.0000 Acc@1: 99.10%\n",
            "| Elapsed time : 0:53:42\n",
            "| Epoch [ 82/200] Iter[938/938]\t\tLoss: 0.1820 Acc@1: 97.383%\n",
            "| Validation Epoch #82\t\t\tLoss: 0.0000 Acc@1: 99.06%\n",
            "| Elapsed time : 0:54:22\n",
            "| Epoch [ 83/200] Iter[938/938]\t\tLoss: 1.0604 Acc@1: 97.280%\n",
            "| Validation Epoch #83\t\t\tLoss: 0.0000 Acc@1: 98.84%\n",
            "| Elapsed time : 0:55:02\n",
            "| Epoch [ 84/200] Iter[938/938]\t\tLoss: 0.1491 Acc@1: 97.287%\n",
            "| Validation Epoch #84\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 0:55:41\n",
            "| Epoch [ 85/200] Iter[938/938]\t\tLoss: 0.0008 Acc@1: 97.250%\n",
            "| Validation Epoch #85\t\t\tLoss: 0.0000 Acc@1: 98.97%\n",
            "| Elapsed time : 0:56:20\n",
            "| Epoch [ 86/200] Iter[938/938]\t\tLoss: 0.0182 Acc@1: 97.307%\n",
            "| Validation Epoch #86\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 0:57:00\n",
            "| Epoch [ 87/200] Iter[938/938]\t\tLoss: 0.0347 Acc@1: 97.203%\n",
            "| Validation Epoch #87\t\t\tLoss: 0.0000 Acc@1: 98.99%\n",
            "| Elapsed time : 0:57:40\n",
            "| Epoch [ 88/200] Iter[938/938]\t\tLoss: 0.0204 Acc@1: 97.423%\n",
            "| Validation Epoch #88\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 0:58:22\n",
            "| Epoch [ 89/200] Iter[938/938]\t\tLoss: 0.0110 Acc@1: 97.280%\n",
            "| Validation Epoch #89\t\t\tLoss: 0.0000 Acc@1: 99.06%\n",
            "| Elapsed time : 0:59:03\n",
            "| Epoch [ 90/200] Iter[938/938]\t\tLoss: 0.3632 Acc@1: 97.335%\n",
            "| Validation Epoch #90\t\t\tLoss: 0.0000 Acc@1: 98.95%\n",
            "| Elapsed time : 0:59:44\n",
            "| Epoch [ 91/200] Iter[938/938]\t\tLoss: 0.0305 Acc@1: 97.302%\n",
            "| Validation Epoch #91\t\t\tLoss: 0.0001 Acc@1: 99.06%\n",
            "| Elapsed time : 1:00:25\n",
            "| Epoch [ 92/200] Iter[938/938]\t\tLoss: 0.1691 Acc@1: 97.340%\n",
            "| Validation Epoch #92\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 1:01:06\n",
            "| Epoch [ 93/200] Iter[938/938]\t\tLoss: 0.0390 Acc@1: 97.343%\n",
            "| Validation Epoch #93\t\t\tLoss: 0.0000 Acc@1: 99.09%\n",
            "| Elapsed time : 1:01:47\n",
            "| Epoch [ 94/200] Iter[938/938]\t\tLoss: 0.0823 Acc@1: 97.360%\n",
            "| Validation Epoch #94\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 1:02:29\n",
            "| Epoch [ 95/200] Iter[938/938]\t\tLoss: 0.0661 Acc@1: 97.418%\n",
            "| Validation Epoch #95\t\t\tLoss: 0.0001 Acc@1: 99.06%\n",
            "| Elapsed time : 1:03:10\n",
            "| Epoch [ 96/200] Iter[938/938]\t\tLoss: 0.0130 Acc@1: 97.402%\n",
            "| Validation Epoch #96\t\t\tLoss: 0.0000 Acc@1: 99.17%\n",
            "| Elapsed time : 1:03:51\n",
            "| Epoch [ 97/200] Iter[938/938]\t\tLoss: 0.0555 Acc@1: 97.490%\n",
            "| Validation Epoch #97\t\t\tLoss: 0.0000 Acc@1: 99.18%\n",
            "| Elapsed time : 1:04:32\n",
            "| Epoch [ 98/200] Iter[938/938]\t\tLoss: 0.1339 Acc@1: 97.418%\n",
            "| Validation Epoch #98\t\t\tLoss: 0.0000 Acc@1: 99.10%\n",
            "| Elapsed time : 1:05:14\n",
            "| Epoch [ 99/200] Iter[938/938]\t\tLoss: 0.2833 Acc@1: 97.430%\n",
            "| Validation Epoch #99\t\t\tLoss: 0.0000 Acc@1: 99.10%\n",
            "| Elapsed time : 1:05:55\n",
            "| Epoch [100/200] Iter[938/938]\t\tLoss: 0.1189 Acc@1: 97.420%\n",
            "| Validation Epoch #100\t\t\tLoss: 0.0000 Acc@1: 99.00%\n",
            "| Elapsed time : 1:06:36\n",
            "| Epoch [101/200] Iter[938/938]\t\tLoss: 0.0249 Acc@1: 97.372%\n",
            "| Validation Epoch #101\t\t\tLoss: 0.0000 Acc@1: 98.97%\n",
            "| Elapsed time : 1:07:17\n",
            "| Epoch [102/200] Iter[938/938]\t\tLoss: 0.1271 Acc@1: 97.443%\n",
            "| Validation Epoch #102\t\t\tLoss: 0.0000 Acc@1: 99.05%\n",
            "| Elapsed time : 1:07:58\n",
            "| Epoch [103/200] Iter[938/938]\t\tLoss: 0.1161 Acc@1: 97.447%\n",
            "| Validation Epoch #103\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 1:08:40\n",
            "| Epoch [104/200] Iter[938/938]\t\tLoss: 0.0485 Acc@1: 97.372%\n",
            "| Validation Epoch #104\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 1:09:21\n",
            "| Epoch [105/200] Iter[938/938]\t\tLoss: 0.0099 Acc@1: 97.480%\n",
            "| Validation Epoch #105\t\t\tLoss: 0.0000 Acc@1: 99.05%\n",
            "| Elapsed time : 1:10:03\n",
            "| Epoch [106/200] Iter[938/938]\t\tLoss: 0.0121 Acc@1: 97.493%\n",
            "| Validation Epoch #106\t\t\tLoss: 0.0000 Acc@1: 99.10%\n",
            "| Elapsed time : 1:10:46\n",
            "| Epoch [107/200] Iter[938/938]\t\tLoss: 0.0555 Acc@1: 97.477%\n",
            "| Validation Epoch #107\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 1:11:28\n",
            "| Epoch [108/200] Iter[938/938]\t\tLoss: 0.0286 Acc@1: 97.542%\n",
            "| Validation Epoch #108\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 1:12:09\n",
            "| Epoch [109/200] Iter[938/938]\t\tLoss: 0.0050 Acc@1: 97.498%\n",
            "| Validation Epoch #109\t\t\tLoss: 0.0000 Acc@1: 99.17%\n",
            "| Elapsed time : 1:12:51\n",
            "| Epoch [110/200] Iter[938/938]\t\tLoss: 0.0031 Acc@1: 97.398%\n",
            "| Validation Epoch #110\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 1:13:32\n",
            "| Epoch [111/200] Iter[938/938]\t\tLoss: 0.0201 Acc@1: 97.420%\n",
            "| Validation Epoch #111\t\t\tLoss: 0.0000 Acc@1: 99.09%\n",
            "| Elapsed time : 1:14:14\n",
            "| Epoch [112/200] Iter[938/938]\t\tLoss: 0.0554 Acc@1: 97.495%\n",
            "| Validation Epoch #112\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 1:14:55\n",
            "| Epoch [113/200] Iter[938/938]\t\tLoss: 0.0270 Acc@1: 97.445%\n",
            "| Validation Epoch #113\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 1:15:37\n",
            "| Epoch [114/200] Iter[938/938]\t\tLoss: 0.0725 Acc@1: 97.432%\n",
            "| Validation Epoch #114\t\t\tLoss: 0.0000 Acc@1: 99.15%\n",
            "| Elapsed time : 1:16:18\n",
            "| Epoch [115/200] Iter[938/938]\t\tLoss: 0.0256 Acc@1: 97.442%\n",
            "| Validation Epoch #115\t\t\tLoss: 0.0000 Acc@1: 99.08%\n",
            "| Elapsed time : 1:17:00\n",
            "| Epoch [116/200] Iter[938/938]\t\tLoss: 0.0336 Acc@1: 97.428%\n",
            "| Validation Epoch #116\t\t\tLoss: 0.0000 Acc@1: 99.01%\n",
            "| Elapsed time : 1:17:41\n",
            "| Epoch [117/200] Iter[938/938]\t\tLoss: 0.1330 Acc@1: 97.320%\n",
            "| Validation Epoch #117\t\t\tLoss: 0.0000 Acc@1: 99.10%\n",
            "| Elapsed time : 1:18:23\n",
            "| Epoch [118/200] Iter[938/938]\t\tLoss: 0.0240 Acc@1: 97.477%\n",
            "| Validation Epoch #118\t\t\tLoss: 0.0001 Acc@1: 99.09%\n",
            "| Elapsed time : 1:19:04\n",
            "| Epoch [119/200] Iter[938/938]\t\tLoss: 0.1467 Acc@1: 97.588%\n",
            "| Validation Epoch #119\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 1:19:46\n",
            "| Epoch [120/200] Iter[938/938]\t\tLoss: 0.0845 Acc@1: 97.502%\n",
            "| Validation Epoch #120\t\t\tLoss: 0.0000 Acc@1: 99.06%\n",
            "| Elapsed time : 1:20:28\n",
            "| Epoch [121/200] Iter[938/938]\t\tLoss: 0.2911 Acc@1: 97.508%\n",
            "| Validation Epoch #121\t\t\tLoss: 0.0000 Acc@1: 99.06%\n",
            "| Elapsed time : 1:21:09\n",
            "| Epoch [122/200] Iter[938/938]\t\tLoss: 0.0287 Acc@1: 97.537%\n",
            "| Validation Epoch #122\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 1:21:51\n",
            "| Epoch [123/200] Iter[938/938]\t\tLoss: 0.0263 Acc@1: 97.530%\n",
            "| Validation Epoch #123\t\t\tLoss: 0.0000 Acc@1: 99.21%\n",
            "| Elapsed time : 1:22:33\n",
            "| Epoch [124/200] Iter[938/938]\t\tLoss: 0.0416 Acc@1: 97.507%\n",
            "| Validation Epoch #124\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 1:23:15\n",
            "| Epoch [125/200] Iter[938/938]\t\tLoss: 0.2680 Acc@1: 97.463%\n",
            "| Validation Epoch #125\t\t\tLoss: 0.0000 Acc@1: 99.14%\n",
            "| Elapsed time : 1:23:56\n",
            "| Epoch [126/200] Iter[938/938]\t\tLoss: 0.1808 Acc@1: 97.527%\n",
            "| Validation Epoch #126\t\t\tLoss: 0.0000 Acc@1: 99.17%\n",
            "| Elapsed time : 1:24:38\n",
            "| Epoch [127/200] Iter[938/938]\t\tLoss: 0.2729 Acc@1: 97.513%\n",
            "| Validation Epoch #127\t\t\tLoss: 0.0000 Acc@1: 99.03%\n",
            "| Elapsed time : 1:25:20\n",
            "| Epoch [128/200] Iter[938/938]\t\tLoss: 0.0106 Acc@1: 97.632%\n",
            "| Validation Epoch #128\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 1:26:01\n",
            "| Epoch [129/200] Iter[938/938]\t\tLoss: 0.0122 Acc@1: 97.477%\n",
            "| Validation Epoch #129\t\t\tLoss: 0.0000 Acc@1: 98.99%\n",
            "| Elapsed time : 1:26:43\n",
            "| Epoch [130/200] Iter[938/938]\t\tLoss: 0.0166 Acc@1: 97.555%\n",
            "| Validation Epoch #130\t\t\tLoss: 0.0000 Acc@1: 99.04%\n",
            "| Elapsed time : 1:27:25\n",
            "| Epoch [131/200] Iter[938/938]\t\tLoss: 0.2096 Acc@1: 97.547%\n",
            "| Validation Epoch #131\t\t\tLoss: 0.0000 Acc@1: 99.16%\n",
            "| Elapsed time : 1:28:06\n",
            "| Epoch [132/200] Iter[938/938]\t\tLoss: 0.0088 Acc@1: 97.602%\n",
            "| Validation Epoch #132\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 1:28:48\n",
            "| Epoch [133/200] Iter[938/938]\t\tLoss: 0.0126 Acc@1: 97.483%\n",
            "| Validation Epoch #133\t\t\tLoss: 0.0000 Acc@1: 99.05%\n",
            "| Elapsed time : 1:29:30\n",
            "| Epoch [134/200] Iter[938/938]\t\tLoss: 0.0706 Acc@1: 97.483%\n",
            "| Validation Epoch #134\t\t\tLoss: 0.0000 Acc@1: 99.09%\n",
            "| Elapsed time : 1:30:12\n",
            "| Epoch [135/200] Iter[938/938]\t\tLoss: 0.0079 Acc@1: 97.510%\n",
            "| Validation Epoch #135\t\t\tLoss: 0.0000 Acc@1: 99.08%\n",
            "| Elapsed time : 1:30:53\n",
            "| Epoch [136/200] Iter[938/938]\t\tLoss: 0.0450 Acc@1: 97.572%\n",
            "| Validation Epoch #136\t\t\tLoss: 0.0000 Acc@1: 99.04%\n",
            "| Elapsed time : 1:31:35\n",
            "| Epoch [137/200] Iter[938/938]\t\tLoss: 0.0454 Acc@1: 97.527%\n",
            "| Validation Epoch #137\t\t\tLoss: 0.0000 Acc@1: 99.09%\n",
            "| Elapsed time : 1:32:17\n",
            "| Epoch [138/200] Iter[938/938]\t\tLoss: 0.1225 Acc@1: 97.583%\n",
            "| Validation Epoch #138\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 1:32:59\n",
            "| Epoch [139/200] Iter[938/938]\t\tLoss: 0.0512 Acc@1: 97.567%\n",
            "| Validation Epoch #139\t\t\tLoss: 0.0000 Acc@1: 99.16%\n",
            "| Elapsed time : 1:33:41\n",
            "| Epoch [140/200] Iter[938/938]\t\tLoss: 0.0665 Acc@1: 97.643%\n",
            "| Validation Epoch #140\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 1:34:23\n",
            "| Epoch [141/200] Iter[938/938]\t\tLoss: 0.0213 Acc@1: 97.567%\n",
            "| Validation Epoch #141\t\t\tLoss: 0.0000 Acc@1: 99.14%\n",
            "| Elapsed time : 1:35:05\n",
            "| Epoch [142/200] Iter[938/938]\t\tLoss: 0.1879 Acc@1: 97.568%\n",
            "| Validation Epoch #142\t\t\tLoss: 0.0000 Acc@1: 99.20%\n",
            "| Elapsed time : 1:35:46\n",
            "| Epoch [143/200] Iter[938/938]\t\tLoss: 0.1118 Acc@1: 97.595%\n",
            "| Validation Epoch #143\t\t\tLoss: 0.0000 Acc@1: 99.14%\n",
            "| Elapsed time : 1:36:28\n",
            "| Epoch [144/200] Iter[938/938]\t\tLoss: 0.0942 Acc@1: 97.682%\n",
            "| Validation Epoch #144\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 1:37:10\n",
            "| Epoch [145/200] Iter[938/938]\t\tLoss: 0.2006 Acc@1: 97.695%\n",
            "| Validation Epoch #145\t\t\tLoss: 0.0000 Acc@1: 99.09%\n",
            "| Elapsed time : 1:37:52\n",
            "| Epoch [146/200] Iter[938/938]\t\tLoss: 0.0413 Acc@1: 97.657%\n",
            "| Validation Epoch #146\t\t\tLoss: 0.0000 Acc@1: 99.14%\n",
            "| Elapsed time : 1:38:34\n",
            "| Epoch [147/200] Iter[938/938]\t\tLoss: 0.0149 Acc@1: 97.577%\n",
            "| Validation Epoch #147\t\t\tLoss: 0.0000 Acc@1: 99.15%\n",
            "| Elapsed time : 1:39:16\n",
            "| Epoch [148/200] Iter[938/938]\t\tLoss: 0.0124 Acc@1: 97.613%\n",
            "| Validation Epoch #148\t\t\tLoss: 0.0000 Acc@1: 99.04%\n",
            "| Elapsed time : 1:40:01\n",
            "| Epoch [149/200] Iter[938/938]\t\tLoss: 0.0049 Acc@1: 97.720%\n",
            "| Validation Epoch #149\t\t\tLoss: 0.0000 Acc@1: 99.16%\n",
            "| Elapsed time : 1:40:43\n",
            "| Epoch [150/200] Iter[938/938]\t\tLoss: 0.2436 Acc@1: 97.675%\n",
            "| Validation Epoch #150\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 1:41:25\n",
            "| Epoch [151/200] Iter[938/938]\t\tLoss: 0.0547 Acc@1: 97.655%\n",
            "| Validation Epoch #151\t\t\tLoss: 0.0000 Acc@1: 99.18%\n",
            "| Elapsed time : 1:42:07\n",
            "| Epoch [152/200] Iter[938/938]\t\tLoss: 0.0429 Acc@1: 97.542%\n",
            "| Validation Epoch #152\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 1:42:49\n",
            "| Epoch [153/200] Iter[938/938]\t\tLoss: 0.0635 Acc@1: 97.628%\n",
            "| Validation Epoch #153\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 1:43:31\n",
            "| Epoch [154/200] Iter[938/938]\t\tLoss: 0.0373 Acc@1: 97.697%\n",
            "| Validation Epoch #154\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 1:44:14\n",
            "| Epoch [155/200] Iter[938/938]\t\tLoss: 0.0848 Acc@1: 97.648%\n",
            "| Validation Epoch #155\t\t\tLoss: 0.0000 Acc@1: 99.16%\n",
            "| Elapsed time : 1:44:56\n",
            "| Epoch [156/200] Iter[938/938]\t\tLoss: 0.0366 Acc@1: 97.520%\n",
            "| Validation Epoch #156\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 1:45:38\n",
            "| Epoch [157/200] Iter[938/938]\t\tLoss: 0.0131 Acc@1: 97.570%\n",
            "| Validation Epoch #157\t\t\tLoss: 0.0000 Acc@1: 99.10%\n",
            "| Elapsed time : 1:46:20\n",
            "| Epoch [158/200] Iter[938/938]\t\tLoss: 0.1215 Acc@1: 97.708%\n",
            "| Validation Epoch #158\t\t\tLoss: 0.0001 Acc@1: 99.16%\n",
            "| Elapsed time : 1:47:02\n",
            "| Epoch [159/200] Iter[938/938]\t\tLoss: 0.0100 Acc@1: 97.633%\n",
            "| Validation Epoch #159\t\t\tLoss: 0.0001 Acc@1: 99.06%\n",
            "| Elapsed time : 1:47:44\n",
            "| Epoch [160/200] Iter[938/938]\t\tLoss: 0.0678 Acc@1: 97.587%\n",
            "| Validation Epoch #160\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 1:48:27\n",
            "| Epoch [161/200] Iter[938/938]\t\tLoss: 0.0595 Acc@1: 97.637%\n",
            "| Validation Epoch #161\t\t\tLoss: 0.0000 Acc@1: 99.09%\n",
            "| Elapsed time : 1:49:09\n",
            "| Epoch [162/200] Iter[938/938]\t\tLoss: 0.1665 Acc@1: 97.613%\n",
            "| Validation Epoch #162\t\t\tLoss: 0.0000 Acc@1: 99.06%\n",
            "| Elapsed time : 1:49:51\n",
            "| Epoch [163/200] Iter[938/938]\t\tLoss: 0.0312 Acc@1: 97.702%\n",
            "| Validation Epoch #163\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 1:50:33\n",
            "| Epoch [164/200] Iter[938/938]\t\tLoss: 0.0496 Acc@1: 97.762%\n",
            "| Validation Epoch #164\t\t\tLoss: 0.0000 Acc@1: 99.15%\n",
            "| Elapsed time : 1:51:15\n",
            "| Epoch [165/200] Iter[938/938]\t\tLoss: 0.0131 Acc@1: 97.722%\n",
            "| Validation Epoch #165\t\t\tLoss: 0.0000 Acc@1: 99.14%\n",
            "| Elapsed time : 1:51:58\n",
            "| Epoch [166/200] Iter[938/938]\t\tLoss: 0.0570 Acc@1: 97.710%\n",
            "| Validation Epoch #166\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 1:52:40\n",
            "| Epoch [167/200] Iter[938/938]\t\tLoss: 0.0182 Acc@1: 97.698%\n",
            "| Validation Epoch #167\t\t\tLoss: 0.0000 Acc@1: 99.16%\n",
            "| Elapsed time : 1:53:22\n",
            "| Epoch [168/200] Iter[938/938]\t\tLoss: 0.0427 Acc@1: 97.752%\n",
            "| Validation Epoch #168\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 1:54:04\n",
            "| Epoch [169/200] Iter[938/938]\t\tLoss: 0.0632 Acc@1: 97.647%\n",
            "| Validation Epoch #169\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 1:54:47\n",
            "| Epoch [170/200] Iter[938/938]\t\tLoss: 0.0561 Acc@1: 97.625%\n",
            "| Validation Epoch #170\t\t\tLoss: 0.0000 Acc@1: 99.18%\n",
            "| Elapsed time : 1:55:29\n",
            "| Epoch [171/200] Iter[938/938]\t\tLoss: 0.4576 Acc@1: 97.672%\n",
            "| Validation Epoch #171\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 1:56:11\n",
            "| Epoch [172/200] Iter[938/938]\t\tLoss: 0.0653 Acc@1: 97.687%\n",
            "| Validation Epoch #172\t\t\tLoss: 0.0000 Acc@1: 99.09%\n",
            "| Elapsed time : 1:56:53\n",
            "| Epoch [173/200] Iter[938/938]\t\tLoss: 0.0121 Acc@1: 97.753%\n",
            "| Validation Epoch #173\t\t\tLoss: 0.0000 Acc@1: 99.06%\n",
            "| Elapsed time : 1:57:36\n",
            "| Epoch [174/200] Iter[938/938]\t\tLoss: 0.0640 Acc@1: 97.743%\n",
            "| Validation Epoch #174\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 1:58:18\n",
            "| Epoch [175/200] Iter[938/938]\t\tLoss: 0.0356 Acc@1: 97.613%\n",
            "| Validation Epoch #175\t\t\tLoss: 0.0000 Acc@1: 99.06%\n",
            "| Elapsed time : 1:59:00\n",
            "| Epoch [176/200] Iter[938/938]\t\tLoss: 0.0242 Acc@1: 97.642%\n",
            "| Validation Epoch #176\t\t\tLoss: 0.0000 Acc@1: 99.04%\n",
            "| Elapsed time : 1:59:43\n",
            "| Epoch [177/200] Iter[938/938]\t\tLoss: 0.0599 Acc@1: 97.700%\n",
            "| Validation Epoch #177\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 2:00:25\n",
            "| Epoch [178/200] Iter[938/938]\t\tLoss: 0.0418 Acc@1: 97.652%\n",
            "| Validation Epoch #178\t\t\tLoss: 0.0000 Acc@1: 99.14%\n",
            "| Elapsed time : 2:01:08\n",
            "| Epoch [179/200] Iter[938/938]\t\tLoss: 0.0161 Acc@1: 97.628%\n",
            "| Validation Epoch #179\t\t\tLoss: 0.0000 Acc@1: 99.15%\n",
            "| Elapsed time : 2:01:50\n",
            "| Epoch [180/200] Iter[938/938]\t\tLoss: 0.3436 Acc@1: 97.800%\n",
            "| Validation Epoch #180\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 2:02:32\n",
            "| Epoch [181/200] Iter[938/938]\t\tLoss: 0.0299 Acc@1: 97.772%\n",
            "| Validation Epoch #181\t\t\tLoss: 0.0000 Acc@1: 99.14%\n",
            "| Elapsed time : 2:03:15\n",
            "| Epoch [182/200] Iter[938/938]\t\tLoss: 0.1728 Acc@1: 97.602%\n",
            "| Validation Epoch #182\t\t\tLoss: 0.0000 Acc@1: 99.09%\n",
            "| Elapsed time : 2:03:57\n",
            "| Epoch [183/200] Iter[938/938]\t\tLoss: 0.1884 Acc@1: 97.680%\n",
            "| Validation Epoch #183\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 2:04:40\n",
            "| Epoch [184/200] Iter[938/938]\t\tLoss: 0.0968 Acc@1: 97.660%\n",
            "| Validation Epoch #184\t\t\tLoss: 0.0000 Acc@1: 99.13%\n",
            "| Elapsed time : 2:05:22\n",
            "| Epoch [185/200] Iter[938/938]\t\tLoss: 0.0189 Acc@1: 97.753%\n",
            "| Validation Epoch #185\t\t\tLoss: 0.0000 Acc@1: 99.08%\n",
            "| Elapsed time : 2:06:05\n",
            "| Epoch [186/200] Iter[938/938]\t\tLoss: 0.1158 Acc@1: 97.695%\n",
            "| Validation Epoch #186\t\t\tLoss: 0.0000 Acc@1: 99.10%\n",
            "| Elapsed time : 2:06:47\n",
            "| Epoch [187/200] Iter[938/938]\t\tLoss: 0.2104 Acc@1: 97.765%\n",
            "| Validation Epoch #187\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 2:07:29\n",
            "| Epoch [188/200] Iter[938/938]\t\tLoss: 0.0782 Acc@1: 97.797%\n",
            "| Validation Epoch #188\t\t\tLoss: 0.0000 Acc@1: 99.15%\n",
            "| Elapsed time : 2:08:12\n",
            "| Epoch [189/200] Iter[938/938]\t\tLoss: 0.0051 Acc@1: 97.717%\n",
            "| Validation Epoch #189\t\t\tLoss: 0.0000 Acc@1: 99.11%\n",
            "| Elapsed time : 2:08:54\n",
            "| Epoch [190/200] Iter[938/938]\t\tLoss: 0.0193 Acc@1: 97.755%\n",
            "| Validation Epoch #190\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 2:09:37\n",
            "| Epoch [191/200] Iter[938/938]\t\tLoss: 0.0526 Acc@1: 97.722%\n",
            "| Validation Epoch #191\t\t\tLoss: 0.0000 Acc@1: 99.18%\n",
            "| Elapsed time : 2:10:19\n",
            "| Epoch [192/200] Iter[938/938]\t\tLoss: 0.0446 Acc@1: 97.837%\n",
            "| Validation Epoch #192\t\t\tLoss: 0.0000 Acc@1: 99.17%\n",
            "| Elapsed time : 2:11:02\n",
            "| Epoch [193/200] Iter[938/938]\t\tLoss: 0.0089 Acc@1: 97.738%\n",
            "| Validation Epoch #193\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 2:11:44\n",
            "| Epoch [194/200] Iter[938/938]\t\tLoss: 0.0813 Acc@1: 97.767%\n",
            "| Validation Epoch #194\t\t\tLoss: 0.0000 Acc@1: 99.16%\n",
            "| Elapsed time : 2:12:27\n",
            "| Epoch [195/200] Iter[938/938]\t\tLoss: 0.0386 Acc@1: 97.700%\n",
            "| Validation Epoch #195\t\t\tLoss: 0.0000 Acc@1: 99.12%\n",
            "| Elapsed time : 2:13:10\n",
            "| Epoch [196/200] Iter[938/938]\t\tLoss: 0.0433 Acc@1: 97.797%\n",
            "| Validation Epoch #196\t\t\tLoss: 0.0000 Acc@1: 99.07%\n",
            "| Elapsed time : 2:13:52\n",
            "| Epoch [197/200] Iter[938/938]\t\tLoss: 0.0067 Acc@1: 97.745%\n",
            "| Validation Epoch #197\t\t\tLoss: 0.0000 Acc@1: 99.17%\n",
            "| Elapsed time : 2:14:35\n",
            "| Epoch [198/200] Iter[938/938]\t\tLoss: 0.3453 Acc@1: 97.662%\n",
            "| Validation Epoch #198\t\t\tLoss: 0.0000 Acc@1: 99.03%\n",
            "| Elapsed time : 2:15:17\n",
            "| Epoch [199/200] Iter[938/938]\t\tLoss: 0.0094 Acc@1: 97.747%\n",
            "| Validation Epoch #199\t\t\tLoss: 0.0000 Acc@1: 99.25%\n",
            "| Elapsed time : 2:16:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REH5NkZ6R1Hx"
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxpeC5mFV9N7"
      },
      "source": [
        "#args\n",
        "\n",
        "def compute_forgetting_statistics(diag_stats, npresentations):\n",
        "\n",
        "    presentations_needed_to_learn = {}\n",
        "    unlearned_per_presentation = {}\n",
        "    margins_per_presentation = {}\n",
        "    first_learned = {}\n",
        "\n",
        "    for example_id, example_stats in diag_stats.items():\n",
        "\n",
        "        # Skip 'train' and 'test' keys of diag_stats\n",
        "        if not isinstance(example_id, str):\n",
        "\n",
        "            # Forgetting event is a transition in accuracy from 1 to 0\n",
        "            presentation_acc = np.array(example_stats[1][:npresentations])\n",
        "            transitions = presentation_acc[1:] - presentation_acc[:-1]\n",
        "\n",
        "            # Find all presentations when forgetting occurs\n",
        "            if len(np.where(transitions == -1)[0]) > 0:\n",
        "                unlearned_per_presentation[example_id] = np.where(\n",
        "                    transitions == -1)[0] + 2\n",
        "            else:\n",
        "                unlearned_per_presentation[example_id] = []\n",
        "\n",
        "            # Find number of presentations needed to learn example, \n",
        "            # e.g. last presentation when acc is 0\n",
        "            if len(np.where(presentation_acc == 0)[0]) > 0:\n",
        "                presentations_needed_to_learn[example_id] = np.where(\n",
        "                    presentation_acc == 0)[0][-1] + 1\n",
        "            else:\n",
        "                presentations_needed_to_learn[example_id] = 0\n",
        "\n",
        "            # Find the misclassication margin for each presentation of the example\n",
        "            margins_per_presentation = np.array(\n",
        "                example_stats[2][:npresentations])\n",
        "\n",
        "            # Find the presentation at which the example was first learned, \n",
        "            # e.g. first presentation when acc is 1\n",
        "            if len(np.where(presentation_acc == 1)[0]) > 0:\n",
        "                first_learned[example_id] = np.where(\n",
        "                    presentation_acc == 1)[0][0]\n",
        "            else:\n",
        "                first_learned[example_id] = np.nan\n",
        "\n",
        "    return presentations_needed_to_learn, unlearned_per_presentation, margins_per_presentation, first_learned\n",
        "\n",
        "\n",
        "# Sorts examples by number of forgetting counts during training, in ascending order\n",
        "# If an example was never learned, it is assigned the maximum number of forgetting counts\n",
        "# If multiple training runs used, sort examples by the sum of their forgetting counts over all runs\n",
        "#\n",
        "# unlearned_per_presentation_all: list of dictionaries, one per training run\n",
        "# first_learned_all: list of dictionaries, one per training run\n",
        "# npresentations: number of training epochs\n",
        "#\n",
        "# Returns 2 numpy arrays containing the sorted example ids and corresponding forgetting counts\n",
        "#\n",
        "def sort_examples_by_forgetting(unlearned_per_presentation_all,\n",
        "                                first_learned_all, npresentations):\n",
        "\n",
        "    # Initialize lists\n",
        "    example_original_order = []\n",
        "    example_stats = []\n",
        "\n",
        "    for example_id in unlearned_per_presentation_all[0].keys():\n",
        "\n",
        "        # Add current example to lists\n",
        "        example_original_order.append(example_id)\n",
        "        example_stats.append(0)\n",
        "\n",
        "        # Iterate over all training runs to calculate the total forgetting count for current example\n",
        "        for i in range(len(unlearned_per_presentation_all)):\n",
        "\n",
        "            # Get all presentations when current example was forgotten during current training run\n",
        "            stats = unlearned_per_presentation_all[i][example_id]\n",
        "\n",
        "            # If example was never learned during current training run, add max forgetting counts\n",
        "            if np.isnan(first_learned_all[i][example_id]):\n",
        "                example_stats[-1] += npresentations\n",
        "            else:\n",
        "                example_stats[-1] += len(stats)\n",
        "\n",
        "    print('Number of unforgettable examples: {}'.format(\n",
        "        len(np.where(np.array(example_stats) == 0)[0])))\n",
        "    return np.array(example_original_order)[np.argsort(\n",
        "        example_stats)], np.sort(example_stats)\n",
        "\n",
        "\n",
        "# Checks whether a given file name matches a list of specified arguments\n",
        "#\n",
        "# fname: string containing file name\n",
        "# args_list: list of strings containing argument names and values, i.e. [arg1, val1, arg2, val2,..]\n",
        "#\n",
        "# Returns 1 if filename matches the filter specified by the argument list, 0 otherwise\n",
        "#\n",
        "def check_filename(fname, args_list):\n",
        "\n",
        "    # # If no arguments are specified to filter by, pass filename\n",
        "    # if args_list is None:\n",
        "    #     return 1\n",
        "\n",
        "    # for arg_ind in list(args_list):#np.arange(0, len(args_list), 2):\n",
        "    #     arg = str(arg_ind)\n",
        "    #     arg_value = str(args_list[arg_ind])\n",
        "\n",
        "    #     # Check if filename matches the current arg and arg value\n",
        "    #     if arg + '_' + arg_value + '__' not in fname:\n",
        "    #         print('skipping file: ' + fname)\n",
        "    #         return 0\n",
        "\n",
        "    return 1\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-1mebn3WLiJ",
        "outputId": "21637626-2c73-410e-98a8-20f03703ec9d"
      },
      "source": [
        "\n",
        "\n",
        "args = {'output_dir': 'mnist_results',\n",
        "        'output_name': 'mnist_sorted',\n",
        "        'input_dir':'mnist_results',\n",
        "        'epochs': 20,\n",
        "        'input_fname_args':\n",
        "        {\n",
        "            'dataset': 'mnist',\n",
        "            'no_droput': False,\n",
        "            'sorting_file': 'none',\n",
        "            'remove_n': 0,\n",
        "            'keep_lowest_n': 0\n",
        "\n",
        "        }\n",
        "\n",
        "        }\n",
        "\n",
        "# Initialize lists to collect forgetting stastics per example across multiple training runs\n",
        "unlearned_per_presentation_all, first_learned_all = [], []\n",
        "\n",
        "for d, _, fs in os.walk(args['input_dir']):\n",
        "    for f in fs:\n",
        "\n",
        "        # Find the files that match input_fname_args and compute forgetting statistics\n",
        "        if f.endswith('stats_dict.pkl') and check_filename(\n",
        "                f, args['input_fname_args']):\n",
        "            print('including file: ' + f)\n",
        "\n",
        "            # Load the dictionary compiled during training run\n",
        "            with open(os.path.join(d, f), 'rb') as fin:\n",
        "                loaded = pickle.load(fin)\n",
        "\n",
        "            # Compute the forgetting statistics per example for training run\n",
        "            _, unlearned_per_presentation, _, first_learned = compute_forgetting_statistics(\n",
        "                loaded, args['epochs'])\n",
        "\n",
        "            unlearned_per_presentation_all.append(\n",
        "                unlearned_per_presentation)\n",
        "            first_learned_all.append(first_learned)\n",
        "\n",
        "if len(unlearned_per_presentation_all) == 0:\n",
        "    print('No input files found in {} that match {}'.format(\n",
        "        args['input_dir'], args['input_fname_args']))\n",
        "else:\n",
        "\n",
        "    # Sort examples by forgetting counts in ascending order, over one or more training runs\n",
        "    ordered_examples, ordered_values = sort_examples_by_forgetting(\n",
        "        unlearned_per_presentation_all, first_learned_all, args['epochs'])\n",
        "\n",
        "    # Save sorted output\n",
        "    if args['output_name'].endswith('.pkl'):\n",
        "        with open(os.path.join(args['output_dir'], args['output_name']),\n",
        "                  'wb') as fout:\n",
        "            pickle.dump({\n",
        "                'indices': ordered_examples,\n",
        "                'forgetting counts': ordered_values\n",
        "            }, fout)\n",
        "    else:\n",
        "        with open(\n",
        "                os.path.join(args['output_dir'], args['output_name'] + '.pkl'),\n",
        "                'wb') as fout:\n",
        "            pickle.dump({\n",
        "                'indices': ordered_examples,\n",
        "                'forgetting counts': ordered_values\n",
        "            }, fout)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "including file: 0__stats_dict.pkl\n",
            "including file: 1__stats_dict.pkl\n",
            "including file: 2__stats_dict.pkl\n",
            "including file: 3__stats_dict.pkl\n",
            "including file: 4__stats_dict.pkl\n",
            "including file: 5__stats_dict.pkl\n",
            "including file: 6__stats_dict.pkl\n",
            "including file: 7__stats_dict.pkl\n",
            "including file: 8__stats_dict.pkl\n",
            "including file: 9__stats_dict.pkl\n",
            "including file: 10__stats_dict.pkl\n",
            "including file: 11__stats_dict.pkl\n",
            "including file: 12__stats_dict.pkl\n",
            "including file: 13__stats_dict.pkl\n",
            "including file: 14__stats_dict.pkl\n",
            "including file: 15__stats_dict.pkl\n",
            "including file: 16__stats_dict.pkl\n",
            "including file: 17__stats_dict.pkl\n",
            "including file: 18__stats_dict.pkl\n",
            "including file: 19__stats_dict.pkl\n",
            "including file: 20__stats_dict.pkl\n",
            "including file: 21__stats_dict.pkl\n",
            "including file: 22__stats_dict.pkl\n",
            "including file: 23__stats_dict.pkl\n",
            "including file: 24__stats_dict.pkl\n",
            "including file: 25__stats_dict.pkl\n",
            "including file: 26__stats_dict.pkl\n",
            "including file: 27__stats_dict.pkl\n",
            "including file: 28__stats_dict.pkl\n",
            "including file: 29__stats_dict.pkl\n",
            "including file: 30__stats_dict.pkl\n",
            "including file: 31__stats_dict.pkl\n",
            "including file: 32__stats_dict.pkl\n",
            "including file: 33__stats_dict.pkl\n",
            "including file: 34__stats_dict.pkl\n",
            "including file: 35__stats_dict.pkl\n",
            "including file: 36__stats_dict.pkl\n",
            "including file: 37__stats_dict.pkl\n",
            "including file: 38__stats_dict.pkl\n",
            "including file: 39__stats_dict.pkl\n",
            "including file: 40__stats_dict.pkl\n",
            "including file: 41__stats_dict.pkl\n",
            "including file: 42__stats_dict.pkl\n",
            "including file: 43__stats_dict.pkl\n",
            "including file: 44__stats_dict.pkl\n",
            "including file: 45__stats_dict.pkl\n",
            "including file: 46__stats_dict.pkl\n",
            "including file: 47__stats_dict.pkl\n",
            "including file: 48__stats_dict.pkl\n",
            "including file: 49__stats_dict.pkl\n",
            "including file: 50__stats_dict.pkl\n",
            "including file: 51__stats_dict.pkl\n",
            "including file: 52__stats_dict.pkl\n",
            "including file: 53__stats_dict.pkl\n",
            "including file: 54__stats_dict.pkl\n",
            "including file: 55__stats_dict.pkl\n",
            "including file: 56__stats_dict.pkl\n",
            "including file: 57__stats_dict.pkl\n",
            "including file: 58__stats_dict.pkl\n",
            "including file: 59__stats_dict.pkl\n",
            "including file: 60__stats_dict.pkl\n",
            "including file: 61__stats_dict.pkl\n",
            "including file: 62__stats_dict.pkl\n",
            "including file: 63__stats_dict.pkl\n",
            "including file: 64__stats_dict.pkl\n",
            "including file: 65__stats_dict.pkl\n",
            "including file: 66__stats_dict.pkl\n",
            "including file: 67__stats_dict.pkl\n",
            "including file: 68__stats_dict.pkl\n",
            "including file: 69__stats_dict.pkl\n",
            "including file: 70__stats_dict.pkl\n",
            "including file: 71__stats_dict.pkl\n",
            "including file: 72__stats_dict.pkl\n",
            "including file: 73__stats_dict.pkl\n",
            "including file: 74__stats_dict.pkl\n",
            "including file: 75__stats_dict.pkl\n",
            "including file: 76__stats_dict.pkl\n",
            "including file: 77__stats_dict.pkl\n",
            "including file: 78__stats_dict.pkl\n",
            "including file: 79__stats_dict.pkl\n",
            "including file: 80__stats_dict.pkl\n",
            "including file: 81__stats_dict.pkl\n",
            "including file: 82__stats_dict.pkl\n",
            "including file: 83__stats_dict.pkl\n",
            "including file: 84__stats_dict.pkl\n",
            "including file: 85__stats_dict.pkl\n",
            "including file: 86__stats_dict.pkl\n",
            "including file: 87__stats_dict.pkl\n",
            "including file: 88__stats_dict.pkl\n",
            "including file: 89__stats_dict.pkl\n",
            "including file: 90__stats_dict.pkl\n",
            "including file: 91__stats_dict.pkl\n",
            "including file: 92__stats_dict.pkl\n",
            "including file: 93__stats_dict.pkl\n",
            "including file: 94__stats_dict.pkl\n",
            "including file: 95__stats_dict.pkl\n",
            "including file: 96__stats_dict.pkl\n",
            "including file: 97__stats_dict.pkl\n",
            "including file: 98__stats_dict.pkl\n",
            "including file: 99__stats_dict.pkl\n",
            "including file: 100__stats_dict.pkl\n",
            "including file: 101__stats_dict.pkl\n",
            "including file: 102__stats_dict.pkl\n",
            "including file: 103__stats_dict.pkl\n",
            "including file: 104__stats_dict.pkl\n",
            "including file: 105__stats_dict.pkl\n",
            "including file: 106__stats_dict.pkl\n",
            "including file: 107__stats_dict.pkl\n",
            "including file: 108__stats_dict.pkl\n",
            "including file: 109__stats_dict.pkl\n",
            "including file: 110__stats_dict.pkl\n",
            "including file: 111__stats_dict.pkl\n",
            "including file: 112__stats_dict.pkl\n",
            "including file: 113__stats_dict.pkl\n",
            "including file: 114__stats_dict.pkl\n",
            "including file: 115__stats_dict.pkl\n",
            "including file: 116__stats_dict.pkl\n",
            "including file: 117__stats_dict.pkl\n",
            "including file: 118__stats_dict.pkl\n",
            "including file: 119__stats_dict.pkl\n",
            "including file: 120__stats_dict.pkl\n",
            "including file: 121__stats_dict.pkl\n",
            "including file: 122__stats_dict.pkl\n",
            "including file: 123__stats_dict.pkl\n",
            "including file: 124__stats_dict.pkl\n",
            "including file: 125__stats_dict.pkl\n",
            "including file: 126__stats_dict.pkl\n",
            "including file: 127__stats_dict.pkl\n",
            "including file: 128__stats_dict.pkl\n",
            "including file: 129__stats_dict.pkl\n",
            "including file: 130__stats_dict.pkl\n",
            "including file: 131__stats_dict.pkl\n",
            "including file: 132__stats_dict.pkl\n",
            "including file: 133__stats_dict.pkl\n",
            "including file: 134__stats_dict.pkl\n",
            "including file: 135__stats_dict.pkl\n",
            "including file: 136__stats_dict.pkl\n",
            "including file: 137__stats_dict.pkl\n",
            "including file: 138__stats_dict.pkl\n",
            "including file: 139__stats_dict.pkl\n",
            "including file: 140__stats_dict.pkl\n",
            "including file: 141__stats_dict.pkl\n",
            "including file: 142__stats_dict.pkl\n",
            "including file: 143__stats_dict.pkl\n",
            "including file: 144__stats_dict.pkl\n",
            "including file: 145__stats_dict.pkl\n",
            "including file: 146__stats_dict.pkl\n",
            "including file: 147__stats_dict.pkl\n",
            "including file: 148__stats_dict.pkl\n",
            "including file: 149__stats_dict.pkl\n",
            "including file: 150__stats_dict.pkl\n",
            "including file: 151__stats_dict.pkl\n",
            "including file: 152__stats_dict.pkl\n",
            "including file: 153__stats_dict.pkl\n",
            "including file: 154__stats_dict.pkl\n",
            "including file: 155__stats_dict.pkl\n",
            "including file: 156__stats_dict.pkl\n",
            "including file: 157__stats_dict.pkl\n",
            "including file: 158__stats_dict.pkl\n",
            "including file: 159__stats_dict.pkl\n",
            "including file: 160__stats_dict.pkl\n",
            "including file: 161__stats_dict.pkl\n",
            "including file: 162__stats_dict.pkl\n",
            "including file: 163__stats_dict.pkl\n",
            "including file: 164__stats_dict.pkl\n",
            "including file: 165__stats_dict.pkl\n",
            "including file: 166__stats_dict.pkl\n",
            "including file: 167__stats_dict.pkl\n",
            "including file: 168__stats_dict.pkl\n",
            "including file: 169__stats_dict.pkl\n",
            "including file: 170__stats_dict.pkl\n",
            "including file: 171__stats_dict.pkl\n",
            "including file: 172__stats_dict.pkl\n",
            "including file: 173__stats_dict.pkl\n",
            "including file: 174__stats_dict.pkl\n",
            "including file: 175__stats_dict.pkl\n",
            "including file: 176__stats_dict.pkl\n",
            "including file: 177__stats_dict.pkl\n",
            "including file: 178__stats_dict.pkl\n",
            "including file: 179__stats_dict.pkl\n",
            "including file: 180__stats_dict.pkl\n",
            "including file: 181__stats_dict.pkl\n",
            "including file: 182__stats_dict.pkl\n",
            "including file: 183__stats_dict.pkl\n",
            "including file: 184__stats_dict.pkl\n",
            "including file: 185__stats_dict.pkl\n",
            "including file: 186__stats_dict.pkl\n",
            "including file: 187__stats_dict.pkl\n",
            "including file: 188__stats_dict.pkl\n",
            "including file: 189__stats_dict.pkl\n",
            "including file: 190__stats_dict.pkl\n",
            "including file: 191__stats_dict.pkl\n",
            "including file: 192__stats_dict.pkl\n",
            "including file: 193__stats_dict.pkl\n",
            "including file: 194__stats_dict.pkl\n",
            "including file: 195__stats_dict.pkl\n",
            "including file: 196__stats_dict.pkl\n",
            "including file: 197__stats_dict.pkl\n",
            "including file: 198__stats_dict.pkl\n",
            "including file: 199__stats_dict.pkl\n",
            "Number of unforgettable examples: 30169\n"
          ]
        }
      ]
    }
  ]
}