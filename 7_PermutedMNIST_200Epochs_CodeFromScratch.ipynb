{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_PermutedMNIST_200Epochs_CodeFromScratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHsPHrDk-Ee0",
        "outputId": "a899ca67-e9a8-461b-aaa5-f2f2f25254fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V35ySJYcCFoc",
        "outputId": "37afdc64-a3a8-4df3-cbec-5e26e628f355"
      },
      "source": [
        "!pwd\n",
        "!ls\n",
        "%cd drive\n",
        "%cd MyDrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "drive  sample_data\n",
            "/content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAVhVb0i-UKU",
        "outputId": "09056e35-f793-4e1c-e248-5eba5aa01741"
      },
      "source": [
        "!git clone https://github.com/mtoneva/example_forgetting.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'example_forgetting'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Total 194 (delta 0), reused 0 (delta 0), pack-reused 194\u001b[K\n",
            "Receiving objects: 100% (194/194), 566.18 KiB | 4.46 MiB/s, done.\n",
            "Resolving deltas: 100% (102/102), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyCLqv4J-tXf",
        "outputId": "4aab4cd3-2593-4679-cdaf-f9f156168d0f"
      },
      "source": [
        "%cd example_forgetting/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/example_forgetting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2swMilJ2-wXq",
        "outputId": "4230537c-f5f6-41ca-f5a4-38db24a926b4"
      },
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
            "Collecting torch==0.4.1.post2\n",
            "  Downloading torch-0.4.1.post2-cp37-cp37m-manylinux1_x86_64.whl (519.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 519.5 MB 21 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.1.8\n",
            "  Downloading torchvision-0.1.8-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from torchvision==0.1.8->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.1.8->-r requirements.txt (line 3)) (1.15.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 0.4.1.post2 which is incompatible.\n",
            "fastai 1.0.61 requires torch>=1.0.0, but you have torch 0.4.1.post2 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-0.4.1.post2 torchvision-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGT9kP5V-0g8"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMCW3TdJC1Sq"
      },
      "source": [
        "# Format time for printing purposes\n",
        "def get_hms(seconds):\n",
        "    m, s = divmod(seconds, 60)\n",
        "    h, m = divmod(m, 60)\n",
        "\n",
        "    return h, m, s\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZdO8pmDC23Z"
      },
      "source": [
        "# Setup basic CNN model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "\n",
        "        if args['no_dropout']:\n",
        "            x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        else:\n",
        "            x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        if not args['no_dropout']:\n",
        "            x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnHCxXY3C4iK"
      },
      "source": [
        "# Train model for one epoch\n",
        "#\n",
        "# example_stats: dictionary containing statistics accumulated over every presentation of example\n",
        "#\n",
        "def train(args, model, device, trainset, optimizer, epoch, example_stats):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    batch_size = args['batch_size']\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Get permutation to shuffle trainset\n",
        "    trainset_permutation_inds = npr.permutation(\n",
        "        np.arange(len(trainset.train_labels)))\n",
        "\n",
        "    for batch_idx, batch_start_ind in enumerate(\n",
        "            range(0, len(trainset.train_labels), batch_size)):\n",
        "\n",
        "        # Get trainset indices for batch\n",
        "        batch_inds = trainset_permutation_inds[batch_start_ind:\n",
        "                                               batch_start_ind + batch_size]\n",
        "\n",
        "        # Get batch inputs and targets, transform them appropriately\n",
        "        transformed_trainset = []\n",
        "        for ind in batch_inds:\n",
        "            transformed_trainset.append(trainset.__getitem__(ind)[0])\n",
        "        inputs = torch.stack(transformed_trainset)\n",
        "        targets = torch.LongTensor(\n",
        "            np.array(trainset.train_labels)[batch_inds].tolist())\n",
        "\n",
        "        # Map to available device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward propagation, compute loss, get predictions\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update statistics and loss\n",
        "        acc = predicted == targets\n",
        "        for j, index in enumerate(batch_inds):\n",
        "\n",
        "            # Get index in original dataset (not sorted by forgetting)\n",
        "            index_in_original_dataset = train_indx[index]\n",
        "\n",
        "            # Compute missclassification margin\n",
        "            output_correct_class = outputs.data[\n",
        "                j, targets[j].item()]  # output for correct class\n",
        "            sorted_output, _ = torch.sort(outputs.data[j, :])\n",
        "            if acc[j]:\n",
        "                # Example classified correctly, highest incorrect class is 2nd largest output\n",
        "                output_highest_incorrect_class = sorted_output[-2]\n",
        "            else:\n",
        "                # Example misclassified, highest incorrect class is max output\n",
        "                output_highest_incorrect_class = sorted_output[-1]\n",
        "            margin = output_correct_class.item(\n",
        "            ) - output_highest_incorrect_class.item()\n",
        "\n",
        "            # Add the statistics of the current training example to dictionary\n",
        "            index_stats = example_stats.get(index_in_original_dataset,\n",
        "                                            [[], [], []])\n",
        "            index_stats[0].append(loss[j].item())\n",
        "            index_stats[1].append(acc[j].sum().item())\n",
        "            index_stats[2].append(margin)\n",
        "            example_stats[index_in_original_dataset] = index_stats\n",
        "\n",
        "        # Update loss, backward propagate, update optimizer\n",
        "        loss = loss.mean()\n",
        "        train_loss += loss.item()\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write(\n",
        "            '| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f Acc@1: %.3f%%' %\n",
        "            (epoch, args['epochs'], batch_idx + 1,\n",
        "             (len(trainset) // batch_size) + 1, loss.item(),\n",
        "             100. * correct.item() / total))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Add training accuracy to dict\n",
        "        index_stats = example_stats.get('train', [[], []])\n",
        "        index_stats[1].append(100. * correct.item() / float(total))\n",
        "        example_stats['train'] = index_stats\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlQdyfzjDPaS"
      },
      "source": [
        "# Evaluate model predictions on heldout test data\n",
        "#\n",
        "# example_stats: dictionary containing statistics accumulated over every presentation of example\n",
        "#\n",
        "def test(args, model, device, testset, example_stats):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_batch_size = 32\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch_idx, batch_start_ind in enumerate(\n",
        "            range(0, len(testset.test_labels), test_batch_size)):\n",
        "\n",
        "        # Get batch inputs and targets\n",
        "        transformed_testset = []\n",
        "        for ind in range(\n",
        "                batch_start_ind,\n",
        "                min(\n",
        "                    len(testset.test_labels),\n",
        "                    batch_start_ind + test_batch_size)):\n",
        "            transformed_testset.append(testset.__getitem__(ind)[0])\n",
        "        inputs = torch.stack(transformed_testset)\n",
        "        targets = torch.LongTensor(\n",
        "            np.array(testset.test_labels)[batch_start_ind:batch_start_ind +\n",
        "                                          test_batch_size].tolist())\n",
        "\n",
        "        # Map to available device\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward propagation, compute loss, get predictions\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss = loss.mean()\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "    # Add test accuracy to dict\n",
        "    acc = 100. * correct.item() / total\n",
        "    index_stats = example_stats.get('test', [[], []])\n",
        "    index_stats[1].append(100. * correct.item() / float(total))\n",
        "    example_stats['test'] = index_stats\n",
        "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f Acc@1: %.2f%%\" %\n",
        "          (epoch, loss.item(), acc))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptLDUWR0DTdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ad492c2-a011-48b5-a415-5999b3fd15cc"
      },
      "source": [
        "args = {'dataset': 'permuted_mnist',\n",
        "        'batch_size': 64,\n",
        "        'epochs':200,\n",
        "        'lr':0.01,\n",
        "        'momentum':0.5,\n",
        "        'no_cuda':False,\n",
        "        'seed':2,\n",
        "        'sorting_file':\"none\",\n",
        "        'remove_n':0,\n",
        "        'keep_lowest_n':0,\n",
        "        'no_dropout':False,\n",
        "        'input_dir':'permuted_mnist_results/',\n",
        "        'output_dir':'permuted_mnist_results/'\n",
        "\n",
        "        }\n",
        "\n",
        "\n",
        "# Set appropriate devices\n",
        "use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "# Set random seed for initialization\n",
        "torch.manual_seed(args['seed'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(args['seed'])\n",
        "npr.seed(args['seed'])\n",
        "\n",
        "# Setup transforms\n",
        "all_transforms = [\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307, ), (0.3081, ))\n",
        "]\n",
        "if args['dataset'] == 'permuted_mnist':\n",
        "    pixel_permutation = torch.randperm(28 * 28)\n",
        "    all_transforms.append(\n",
        "        transforms.Lambda(\n",
        "            lambda x: x.view(-1, 1)[pixel_permutation].view(1, 28, 28)))\n",
        "transform = transforms.Compose(all_transforms)\n",
        "\n",
        "os.makedirs(args['output_dir'], exist_ok=True)\n",
        "\n",
        "# Load the appropriate train and test datasets\n",
        "trainset = datasets.MNIST(\n",
        "    root='/tmp/data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(\n",
        "    root='/tmp/data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Get indices of examples that should be used for training\n",
        "if args['sorting_file'] == 'none':\n",
        "    train_indx = np.array(range(len(trainset.train_labels)))\n",
        "else:\n",
        "    try:\n",
        "        with open(\n",
        "                os.path.join(args['input_dir'], args['sorting_file']) + '.pkl',\n",
        "                'rb') as fin:\n",
        "            ordered_indx = pickle.load(fin)['indices']\n",
        "    except IOError:\n",
        "        with open(os.path.join(args['input_dir'], args['sorting_file']),\n",
        "                  'rb') as fin:\n",
        "            ordered_indx = pickle.load(fin)['indices']\n",
        "\n",
        "    # Get the indices to remove from training\n",
        "    elements_to_remove = np.array(\n",
        "        ordered_indx)[args['keep_lowest_n']:args['keep_lowest_n'] + args['remove_n']]\n",
        "\n",
        "    # Remove the corresponding elements\n",
        "    train_indx = np.setdiff1d(\n",
        "        range(len(trainset.train_labels)), elements_to_remove)\n",
        "\n",
        "# Remove remove_n number of examples from the train set at random\n",
        "if args['keep_lowest_n'] < 0:\n",
        "    train_indx = npr.permutation(np.arange(len(\n",
        "        trainset.train_labels)))[:len(trainset.train_labels) - args['remove_n']]\n",
        "\n",
        "# Reassign train data and labels\n",
        "trainset.train_data = trainset.train_data[train_indx, :, :]\n",
        "trainset.train_labels = np.array(trainset.train_labels)[train_indx].tolist()\n",
        "\n",
        "print('Training on ' + str(len(trainset.train_labels)) + ' examples')\n",
        "\n",
        "# Setup model and optimizer\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "# Setup loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.__init__(reduce=False)\n",
        "\n",
        "# Initialize dictionary to save statistics for every example presentation\n",
        "example_stats = {}\n",
        "\n",
        "elapsed_time = 0"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Training on 60000 examples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyS_EDlHEZYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756edb9c-2ad3-48c5-f6b0-3c076b64ad78"
      },
      "source": [
        "for epoch in range(args['epochs']):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train(args, model, device, trainset, optimizer, epoch, example_stats)\n",
        "    test(args, model, device, testset, example_stats)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    elapsed_time += epoch_time\n",
        "    print('| Elapsed time : %d:%02d:%02d' % (get_hms(elapsed_time)))\n",
        "\n",
        "    # Save the stats dictionary\n",
        "    fname = os.path.join(args['output_dir'], str(epoch))\n",
        "    with open(fname + \"__stats_dict.pkl\", \"wb\") as f:\n",
        "        pickle.dump(example_stats, f)\n",
        "\n",
        "    # Log the best train and test accuracy so far\n",
        "    with open(fname + \"__best_acc.txt\", \"w\") as f:\n",
        "        f.write('train test \\n')\n",
        "        f.write(str(max(example_stats['train'][1])))\n",
        "        f.write(' ')\n",
        "        f.write(str(max(example_stats['test'][1])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch [  0/200] Iter[938/938]\t\tLoss: 0.7132 Acc@1: 43.438%\n",
            "| Validation Epoch #0\t\t\tLoss: 0.3346 Acc@1: 83.98%\n",
            "| Elapsed time : 0:00:42\n",
            "| Epoch [  1/200] Iter[938/938]\t\tLoss: 0.4494 Acc@1: 77.578%\n",
            "| Validation Epoch #1\t\t\tLoss: 0.0949 Acc@1: 90.24%\n",
            "| Elapsed time : 0:01:24\n",
            "| Epoch [  2/200] Iter[938/938]\t\tLoss: 0.5114 Acc@1: 82.973%\n",
            "| Validation Epoch #2\t\t\tLoss: 0.0658 Acc@1: 91.82%\n",
            "| Elapsed time : 0:02:07\n",
            "| Epoch [  3/200] Iter[938/938]\t\tLoss: 0.3167 Acc@1: 85.455%\n",
            "| Validation Epoch #3\t\t\tLoss: 0.0581 Acc@1: 92.72%\n",
            "| Elapsed time : 0:02:50\n",
            "| Epoch [  4/200] Iter[938/938]\t\tLoss: 0.5026 Acc@1: 86.623%\n",
            "| Validation Epoch #4\t\t\tLoss: 0.0406 Acc@1: 93.10%\n",
            "| Elapsed time : 0:03:32\n",
            "| Epoch [  5/200] Iter[938/938]\t\tLoss: 0.3511 Acc@1: 87.455%\n",
            "| Validation Epoch #5\t\t\tLoss: 0.0369 Acc@1: 93.60%\n",
            "| Elapsed time : 0:04:14\n",
            "| Epoch [  6/200] Iter[938/938]\t\tLoss: 0.2670 Acc@1: 87.952%\n",
            "| Validation Epoch #6\t\t\tLoss: 0.0231 Acc@1: 93.42%\n",
            "| Elapsed time : 0:04:57\n",
            "| Epoch [  7/200] Iter[938/938]\t\tLoss: 0.4480 Acc@1: 88.503%\n",
            "| Validation Epoch #7\t\t\tLoss: 0.0173 Acc@1: 93.93%\n",
            "| Elapsed time : 0:05:40\n",
            "| Epoch [  8/200] Iter[938/938]\t\tLoss: 0.5012 Acc@1: 88.890%\n",
            "| Validation Epoch #8\t\t\tLoss: 0.0130 Acc@1: 94.03%\n",
            "| Elapsed time : 0:06:23\n",
            "| Epoch [  9/200] Iter[938/938]\t\tLoss: 0.2198 Acc@1: 89.038%\n",
            "| Validation Epoch #9\t\t\tLoss: 0.0076 Acc@1: 94.21%\n",
            "| Elapsed time : 0:07:06\n",
            "| Epoch [ 10/200] Iter[938/938]\t\tLoss: 0.3732 Acc@1: 89.445%\n",
            "| Validation Epoch #10\t\t\tLoss: 0.0075 Acc@1: 94.21%\n",
            "| Elapsed time : 0:07:49\n",
            "| Epoch [ 11/200] Iter[938/938]\t\tLoss: 0.6879 Acc@1: 89.492%\n",
            "| Validation Epoch #11\t\t\tLoss: 0.0108 Acc@1: 94.35%\n",
            "| Elapsed time : 0:08:31\n",
            "| Epoch [ 12/200] Iter[938/938]\t\tLoss: 0.6729 Acc@1: 89.792%\n",
            "| Validation Epoch #12\t\t\tLoss: 0.0152 Acc@1: 94.52%\n",
            "| Elapsed time : 0:09:14\n",
            "| Epoch [ 13/200] Iter[938/938]\t\tLoss: 0.3045 Acc@1: 89.760%\n",
            "| Validation Epoch #13\t\t\tLoss: 0.0101 Acc@1: 94.49%\n",
            "| Elapsed time : 0:09:57\n",
            "| Epoch [ 14/200] Iter[938/938]\t\tLoss: 0.4796 Acc@1: 89.902%\n",
            "| Validation Epoch #14\t\t\tLoss: 0.0162 Acc@1: 94.69%\n",
            "| Elapsed time : 0:10:40\n",
            "| Epoch [ 15/200] Iter[938/938]\t\tLoss: 0.3743 Acc@1: 90.188%\n",
            "| Validation Epoch #15\t\t\tLoss: 0.0050 Acc@1: 94.52%\n",
            "| Elapsed time : 0:11:22\n",
            "| Epoch [ 16/200] Iter[938/938]\t\tLoss: 0.1594 Acc@1: 90.175%\n",
            "| Validation Epoch #16\t\t\tLoss: 0.0051 Acc@1: 94.66%\n",
            "| Elapsed time : 0:12:06\n",
            "| Epoch [ 17/200] Iter[938/938]\t\tLoss: 0.5766 Acc@1: 90.117%\n",
            "| Validation Epoch #17\t\t\tLoss: 0.0146 Acc@1: 94.77%\n",
            "| Elapsed time : 0:12:48\n",
            "| Epoch [ 18/200] Iter[938/938]\t\tLoss: 0.1669 Acc@1: 90.220%\n",
            "| Validation Epoch #18\t\t\tLoss: 0.0076 Acc@1: 94.77%\n",
            "| Elapsed time : 0:13:31\n",
            "| Epoch [ 19/200] Iter[938/938]\t\tLoss: 0.3626 Acc@1: 90.470%\n",
            "| Validation Epoch #19\t\t\tLoss: 0.0052 Acc@1: 94.74%\n",
            "| Elapsed time : 0:14:14\n",
            "| Epoch [ 20/200] Iter[938/938]\t\tLoss: 0.3093 Acc@1: 90.498%\n",
            "| Validation Epoch #20\t\t\tLoss: 0.0072 Acc@1: 94.72%\n",
            "| Elapsed time : 0:14:57\n",
            "| Epoch [ 21/200] Iter[938/938]\t\tLoss: 0.4083 Acc@1: 90.587%\n",
            "| Validation Epoch #21\t\t\tLoss: 0.0061 Acc@1: 94.81%\n",
            "| Elapsed time : 0:15:40\n",
            "| Epoch [ 22/200] Iter[938/938]\t\tLoss: 0.4322 Acc@1: 90.565%\n",
            "| Validation Epoch #22\t\t\tLoss: 0.0052 Acc@1: 94.78%\n",
            "| Elapsed time : 0:16:23\n",
            "| Epoch [ 23/200] Iter[938/938]\t\tLoss: 0.2133 Acc@1: 90.722%\n",
            "| Validation Epoch #23\t\t\tLoss: 0.0066 Acc@1: 94.96%\n",
            "| Elapsed time : 0:17:07\n",
            "| Epoch [ 24/200] Iter[938/938]\t\tLoss: 0.2213 Acc@1: 90.677%\n",
            "| Validation Epoch #24\t\t\tLoss: 0.0059 Acc@1: 94.95%\n",
            "| Elapsed time : 0:17:49\n",
            "| Epoch [ 25/200] Iter[938/938]\t\tLoss: 0.0902 Acc@1: 90.877%\n",
            "| Validation Epoch #25\t\t\tLoss: 0.0045 Acc@1: 94.89%\n",
            "| Elapsed time : 0:18:33\n",
            "| Epoch [ 26/200] Iter[938/938]\t\tLoss: 0.5377 Acc@1: 90.757%\n",
            "| Validation Epoch #26\t\t\tLoss: 0.0046 Acc@1: 95.03%\n",
            "| Elapsed time : 0:19:16\n",
            "| Epoch [ 27/200] Iter[938/938]\t\tLoss: 0.2974 Acc@1: 90.888%\n",
            "| Validation Epoch #27\t\t\tLoss: 0.0038 Acc@1: 95.21%\n",
            "| Elapsed time : 0:19:59\n",
            "| Epoch [ 28/200] Iter[938/938]\t\tLoss: 0.2392 Acc@1: 91.038%\n",
            "| Validation Epoch #28\t\t\tLoss: 0.0050 Acc@1: 94.94%\n",
            "| Elapsed time : 0:20:42\n",
            "| Epoch [ 29/200] Iter[938/938]\t\tLoss: 0.3789 Acc@1: 90.932%\n",
            "| Validation Epoch #29\t\t\tLoss: 0.0055 Acc@1: 94.97%\n",
            "| Elapsed time : 0:21:25\n",
            "| Epoch [ 30/200] Iter[938/938]\t\tLoss: 0.2578 Acc@1: 91.145%\n",
            "| Validation Epoch #30\t\t\tLoss: 0.0038 Acc@1: 94.95%\n",
            "| Elapsed time : 0:22:10\n",
            "| Epoch [ 31/200] Iter[938/938]\t\tLoss: 0.5644 Acc@1: 91.150%\n",
            "| Validation Epoch #31\t\t\tLoss: 0.0058 Acc@1: 94.97%\n",
            "| Elapsed time : 0:22:54\n",
            "| Epoch [ 32/200] Iter[938/938]\t\tLoss: 0.2988 Acc@1: 91.162%\n",
            "| Validation Epoch #32\t\t\tLoss: 0.0072 Acc@1: 95.06%\n",
            "| Elapsed time : 0:23:38\n",
            "| Epoch [ 33/200] Iter[938/938]\t\tLoss: 0.5655 Acc@1: 91.187%\n",
            "| Validation Epoch #33\t\t\tLoss: 0.0072 Acc@1: 95.15%\n",
            "| Elapsed time : 0:24:21\n",
            "| Epoch [ 34/200] Iter[938/938]\t\tLoss: 0.2316 Acc@1: 91.150%\n",
            "| Validation Epoch #34\t\t\tLoss: 0.0056 Acc@1: 95.08%\n",
            "| Elapsed time : 0:25:04\n",
            "| Epoch [ 35/200] Iter[938/938]\t\tLoss: 0.2276 Acc@1: 91.213%\n",
            "| Validation Epoch #35\t\t\tLoss: 0.0051 Acc@1: 95.19%\n",
            "| Elapsed time : 0:25:47\n",
            "| Epoch [ 36/200] Iter[938/938]\t\tLoss: 0.0945 Acc@1: 91.122%\n",
            "| Validation Epoch #36\t\t\tLoss: 0.0068 Acc@1: 95.11%\n",
            "| Elapsed time : 0:26:31\n",
            "| Epoch [ 37/200] Iter[938/938]\t\tLoss: 0.1334 Acc@1: 91.238%\n",
            "| Validation Epoch #37\t\t\tLoss: 0.0103 Acc@1: 95.18%\n",
            "| Elapsed time : 0:27:14\n",
            "| Epoch [ 38/200] Iter[938/938]\t\tLoss: 0.1443 Acc@1: 91.200%\n",
            "| Validation Epoch #38\t\t\tLoss: 0.0050 Acc@1: 95.12%\n",
            "| Elapsed time : 0:27:59\n",
            "| Epoch [ 39/200] Iter[938/938]\t\tLoss: 0.2204 Acc@1: 91.200%\n",
            "| Validation Epoch #39\t\t\tLoss: 0.0066 Acc@1: 95.28%\n",
            "| Elapsed time : 0:28:42\n",
            "| Epoch [ 40/200] Iter[938/938]\t\tLoss: 0.0644 Acc@1: 91.205%\n",
            "| Validation Epoch #40\t\t\tLoss: 0.0049 Acc@1: 95.09%\n",
            "| Elapsed time : 0:29:26\n",
            "| Epoch [ 41/200] Iter[938/938]\t\tLoss: 0.3703 Acc@1: 91.258%\n",
            "| Validation Epoch #41\t\t\tLoss: 0.0032 Acc@1: 95.27%\n",
            "| Elapsed time : 0:30:09\n",
            "| Epoch [ 42/200] Iter[938/938]\t\tLoss: 0.3204 Acc@1: 91.402%\n",
            "| Validation Epoch #42\t\t\tLoss: 0.0027 Acc@1: 95.34%\n",
            "| Elapsed time : 0:30:53\n",
            "| Epoch [ 43/200] Iter[938/938]\t\tLoss: 0.1535 Acc@1: 91.227%\n",
            "| Validation Epoch #43\t\t\tLoss: 0.0051 Acc@1: 95.20%\n",
            "| Elapsed time : 0:31:36\n",
            "| Epoch [ 44/200] Iter[938/938]\t\tLoss: 0.1236 Acc@1: 91.310%\n",
            "| Validation Epoch #44\t\t\tLoss: 0.0043 Acc@1: 95.21%\n",
            "| Elapsed time : 0:32:19\n",
            "| Epoch [ 45/200] Iter[938/938]\t\tLoss: 0.3820 Acc@1: 91.275%\n",
            "| Validation Epoch #45\t\t\tLoss: 0.0030 Acc@1: 95.30%\n",
            "| Elapsed time : 0:33:03\n",
            "| Epoch [ 46/200] Iter[938/938]\t\tLoss: 0.2189 Acc@1: 91.345%\n",
            "| Validation Epoch #46\t\t\tLoss: 0.0042 Acc@1: 95.26%\n",
            "| Elapsed time : 0:33:46\n",
            "| Epoch [ 47/200] Iter[938/938]\t\tLoss: 0.1227 Acc@1: 91.358%\n",
            "| Validation Epoch #47\t\t\tLoss: 0.0030 Acc@1: 95.07%\n",
            "| Elapsed time : 0:34:30\n",
            "| Epoch [ 48/200] Iter[938/938]\t\tLoss: 0.3088 Acc@1: 91.550%\n",
            "| Validation Epoch #48\t\t\tLoss: 0.0037 Acc@1: 95.32%\n",
            "| Elapsed time : 0:35:13\n",
            "| Epoch [ 49/200] Iter[938/938]\t\tLoss: 0.4278 Acc@1: 91.378%\n",
            "| Validation Epoch #49\t\t\tLoss: 0.0061 Acc@1: 95.16%\n",
            "| Elapsed time : 0:35:57\n",
            "| Epoch [ 50/200] Iter[938/938]\t\tLoss: 0.5697 Acc@1: 91.447%\n",
            "| Validation Epoch #50\t\t\tLoss: 0.0079 Acc@1: 95.16%\n",
            "| Elapsed time : 0:36:41\n",
            "| Epoch [ 51/200] Iter[938/938]\t\tLoss: 0.1759 Acc@1: 91.478%\n",
            "| Validation Epoch #51\t\t\tLoss: 0.0016 Acc@1: 95.38%\n",
            "| Elapsed time : 0:37:25\n",
            "| Epoch [ 52/200] Iter[938/938]\t\tLoss: 0.2893 Acc@1: 91.553%\n",
            "| Validation Epoch #52\t\t\tLoss: 0.0030 Acc@1: 95.25%\n",
            "| Elapsed time : 0:38:09\n",
            "| Epoch [ 53/200] Iter[938/938]\t\tLoss: 0.2635 Acc@1: 91.505%\n",
            "| Validation Epoch #53\t\t\tLoss: 0.0033 Acc@1: 95.33%\n",
            "| Elapsed time : 0:38:53\n",
            "| Epoch [ 54/200] Iter[938/938]\t\tLoss: 0.4705 Acc@1: 91.388%\n",
            "| Validation Epoch #54\t\t\tLoss: 0.0025 Acc@1: 95.31%\n",
            "| Elapsed time : 0:39:37\n",
            "| Epoch [ 55/200] Iter[938/938]\t\tLoss: 0.3070 Acc@1: 91.645%\n",
            "| Validation Epoch #55\t\t\tLoss: 0.0045 Acc@1: 95.20%\n",
            "| Elapsed time : 0:40:21\n",
            "| Epoch [ 56/200] Iter[938/938]\t\tLoss: 0.2565 Acc@1: 91.432%\n",
            "| Validation Epoch #56\t\t\tLoss: 0.0044 Acc@1: 95.30%\n",
            "| Elapsed time : 0:41:05\n",
            "| Epoch [ 57/200] Iter[938/938]\t\tLoss: 0.2365 Acc@1: 91.535%\n",
            "| Validation Epoch #57\t\t\tLoss: 0.0029 Acc@1: 95.20%\n",
            "| Elapsed time : 0:41:49\n",
            "| Epoch [ 58/200] Iter[938/938]\t\tLoss: 0.4242 Acc@1: 91.602%\n",
            "| Validation Epoch #58\t\t\tLoss: 0.0019 Acc@1: 95.28%\n",
            "| Elapsed time : 0:42:34\n",
            "| Epoch [ 59/200] Iter[938/938]\t\tLoss: 0.5070 Acc@1: 91.677%\n",
            "| Validation Epoch #59\t\t\tLoss: 0.0069 Acc@1: 95.23%\n",
            "| Elapsed time : 0:43:18\n",
            "| Epoch [ 60/200] Iter[938/938]\t\tLoss: 0.2057 Acc@1: 91.707%\n",
            "| Validation Epoch #60\t\t\tLoss: 0.0061 Acc@1: 95.41%\n",
            "| Elapsed time : 0:44:02\n",
            "| Epoch [ 61/200] Iter[938/938]\t\tLoss: 0.8032 Acc@1: 91.575%\n",
            "| Validation Epoch #61\t\t\tLoss: 0.0037 Acc@1: 95.40%\n",
            "| Elapsed time : 0:44:45\n",
            "| Epoch [ 62/200] Iter[938/938]\t\tLoss: 0.1113 Acc@1: 91.425%\n",
            "| Validation Epoch #62\t\t\tLoss: 0.0018 Acc@1: 95.18%\n",
            "| Elapsed time : 0:45:29\n",
            "| Epoch [ 63/200] Iter[938/938]\t\tLoss: 0.3766 Acc@1: 91.498%\n",
            "| Validation Epoch #63\t\t\tLoss: 0.0022 Acc@1: 95.64%\n",
            "| Elapsed time : 0:46:12\n",
            "| Epoch [ 64/200] Iter[938/938]\t\tLoss: 0.4495 Acc@1: 91.558%\n",
            "| Validation Epoch #64\t\t\tLoss: 0.0024 Acc@1: 95.21%\n",
            "| Elapsed time : 0:46:56\n",
            "| Epoch [ 65/200] Iter[938/938]\t\tLoss: 0.4967 Acc@1: 91.613%\n",
            "| Validation Epoch #65\t\t\tLoss: 0.0048 Acc@1: 95.39%\n",
            "| Elapsed time : 0:47:40\n",
            "| Epoch [ 66/200] Iter[938/938]\t\tLoss: 0.2238 Acc@1: 91.777%\n",
            "| Validation Epoch #66\t\t\tLoss: 0.0058 Acc@1: 95.54%\n",
            "| Elapsed time : 0:48:25\n",
            "| Epoch [ 67/200] Iter[938/938]\t\tLoss: 0.1811 Acc@1: 91.722%\n",
            "| Validation Epoch #67\t\t\tLoss: 0.0034 Acc@1: 95.51%\n",
            "| Elapsed time : 0:49:08\n",
            "| Epoch [ 68/200] Iter[938/938]\t\tLoss: 0.2520 Acc@1: 91.730%\n",
            "| Validation Epoch #68\t\t\tLoss: 0.0031 Acc@1: 95.49%\n",
            "| Elapsed time : 0:49:51\n",
            "| Epoch [ 69/200] Iter[938/938]\t\tLoss: 0.3315 Acc@1: 91.852%\n",
            "| Validation Epoch #69\t\t\tLoss: 0.0034 Acc@1: 95.43%\n",
            "| Elapsed time : 0:50:35\n",
            "| Epoch [ 70/200] Iter[938/938]\t\tLoss: 0.1517 Acc@1: 91.775%\n",
            "| Validation Epoch #70\t\t\tLoss: 0.0047 Acc@1: 95.55%\n",
            "| Elapsed time : 0:51:18\n",
            "| Epoch [ 71/200] Iter[938/938]\t\tLoss: 0.6963 Acc@1: 91.845%\n",
            "| Validation Epoch #71\t\t\tLoss: 0.0083 Acc@1: 95.41%\n",
            "| Elapsed time : 0:52:01\n",
            "| Epoch [ 72/200] Iter[938/938]\t\tLoss: 0.2038 Acc@1: 91.732%\n",
            "| Validation Epoch #72\t\t\tLoss: 0.0050 Acc@1: 95.50%\n",
            "| Elapsed time : 0:52:46\n",
            "| Epoch [ 73/200] Iter[938/938]\t\tLoss: 0.2613 Acc@1: 91.770%\n",
            "| Validation Epoch #73\t\t\tLoss: 0.0108 Acc@1: 95.53%\n",
            "| Elapsed time : 0:53:28\n",
            "| Epoch [ 74/200] Iter[938/938]\t\tLoss: 0.2615 Acc@1: 91.738%\n",
            "| Validation Epoch #74\t\t\tLoss: 0.0025 Acc@1: 95.42%\n",
            "| Elapsed time : 0:54:11\n",
            "| Epoch [ 75/200] Iter[938/938]\t\tLoss: 0.1381 Acc@1: 91.795%\n",
            "| Validation Epoch #75\t\t\tLoss: 0.0031 Acc@1: 95.54%\n",
            "| Elapsed time : 0:54:54\n",
            "| Epoch [ 76/200] Iter[938/938]\t\tLoss: 0.3215 Acc@1: 91.955%\n",
            "| Validation Epoch #76\t\t\tLoss: 0.0033 Acc@1: 95.44%\n",
            "| Elapsed time : 0:55:38\n",
            "| Epoch [ 77/200] Iter[938/938]\t\tLoss: 0.2862 Acc@1: 91.825%\n",
            "| Validation Epoch #77\t\t\tLoss: 0.0025 Acc@1: 95.53%\n",
            "| Elapsed time : 0:56:21\n",
            "| Epoch [ 78/200] Iter[938/938]\t\tLoss: 0.2855 Acc@1: 91.910%\n",
            "| Validation Epoch #78\t\t\tLoss: 0.0029 Acc@1: 95.60%\n",
            "| Elapsed time : 0:57:05\n",
            "| Epoch [ 79/200] Iter[938/938]\t\tLoss: 0.2700 Acc@1: 91.890%\n",
            "| Validation Epoch #79\t\t\tLoss: 0.0076 Acc@1: 95.53%\n",
            "| Elapsed time : 0:57:50\n",
            "| Epoch [ 80/200] Iter[938/938]\t\tLoss: 0.2867 Acc@1: 91.802%\n",
            "| Validation Epoch #80\t\t\tLoss: 0.0058 Acc@1: 95.34%\n",
            "| Elapsed time : 0:58:34\n",
            "| Epoch [ 81/200] Iter[938/938]\t\tLoss: 0.6179 Acc@1: 91.973%\n",
            "| Validation Epoch #81\t\t\tLoss: 0.0104 Acc@1: 95.48%\n",
            "| Elapsed time : 0:59:20\n",
            "| Epoch [ 82/200] Iter[938/938]\t\tLoss: 0.2779 Acc@1: 91.810%\n",
            "| Validation Epoch #82\t\t\tLoss: 0.0031 Acc@1: 95.55%\n",
            "| Elapsed time : 1:00:05\n",
            "| Epoch [ 83/200] Iter[938/938]\t\tLoss: 0.6217 Acc@1: 92.077%\n",
            "| Validation Epoch #83\t\t\tLoss: 0.0035 Acc@1: 95.54%\n",
            "| Elapsed time : 1:00:49\n",
            "| Epoch [ 84/200] Iter[938/938]\t\tLoss: 0.2212 Acc@1: 91.878%\n",
            "| Validation Epoch #84\t\t\tLoss: 0.0052 Acc@1: 95.56%\n",
            "| Elapsed time : 1:01:34\n",
            "| Epoch [ 85/200] Iter[938/938]\t\tLoss: 0.2028 Acc@1: 91.782%\n",
            "| Validation Epoch #85\t\t\tLoss: 0.0036 Acc@1: 95.52%\n",
            "| Elapsed time : 1:02:19\n",
            "| Epoch [ 86/200] Iter[938/938]\t\tLoss: 0.1925 Acc@1: 91.858%\n",
            "| Validation Epoch #86\t\t\tLoss: 0.0058 Acc@1: 95.56%\n",
            "| Elapsed time : 1:03:05\n",
            "| Epoch [ 87/200] Iter[938/938]\t\tLoss: 0.0773 Acc@1: 91.788%\n",
            "| Validation Epoch #87\t\t\tLoss: 0.0039 Acc@1: 95.66%\n",
            "| Elapsed time : 1:03:49\n",
            "| Epoch [ 88/200] Iter[938/938]\t\tLoss: 0.1242 Acc@1: 91.902%\n",
            "| Validation Epoch #88\t\t\tLoss: 0.0040 Acc@1: 95.57%\n",
            "| Elapsed time : 1:04:36\n",
            "| Epoch [ 89/200] Iter[938/938]\t\tLoss: 0.4728 Acc@1: 91.905%\n",
            "| Validation Epoch #89\t\t\tLoss: 0.0016 Acc@1: 95.58%\n",
            "| Elapsed time : 1:05:21\n",
            "| Epoch [ 90/200] Iter[938/938]\t\tLoss: 1.0184 Acc@1: 91.922%\n",
            "| Validation Epoch #90\t\t\tLoss: 0.0020 Acc@1: 95.46%\n",
            "| Elapsed time : 1:06:07\n",
            "| Epoch [ 91/200] Iter[938/938]\t\tLoss: 0.3550 Acc@1: 91.862%\n",
            "| Validation Epoch #91\t\t\tLoss: 0.0089 Acc@1: 95.55%\n",
            "| Elapsed time : 1:06:51\n",
            "| Epoch [ 92/200] Iter[938/938]\t\tLoss: 0.2150 Acc@1: 91.840%\n",
            "| Validation Epoch #92\t\t\tLoss: 0.0057 Acc@1: 95.51%\n",
            "| Elapsed time : 1:07:37\n",
            "| Epoch [ 93/200] Iter[938/938]\t\tLoss: 0.1839 Acc@1: 91.917%\n",
            "| Validation Epoch #93\t\t\tLoss: 0.0040 Acc@1: 95.68%\n",
            "| Elapsed time : 1:08:23\n",
            "| Epoch [ 94/200] Iter[938/938]\t\tLoss: 0.5898 Acc@1: 91.982%\n",
            "| Validation Epoch #94\t\t\tLoss: 0.0040 Acc@1: 95.46%\n",
            "| Elapsed time : 1:09:08\n",
            "| Epoch [ 95/200] Iter[938/938]\t\tLoss: 0.4811 Acc@1: 91.935%\n",
            "| Validation Epoch #95\t\t\tLoss: 0.0028 Acc@1: 95.57%\n",
            "| Elapsed time : 1:09:53\n",
            "| Epoch [ 96/200] Iter[938/938]\t\tLoss: 0.0381 Acc@1: 92.018%\n",
            "| Validation Epoch #96\t\t\tLoss: 0.0041 Acc@1: 95.67%\n",
            "| Elapsed time : 1:10:38\n",
            "| Epoch [ 97/200] Iter[938/938]\t\tLoss: 0.5356 Acc@1: 92.093%\n",
            "| Validation Epoch #97\t\t\tLoss: 0.0071 Acc@1: 95.52%\n",
            "| Elapsed time : 1:11:23\n",
            "| Epoch [ 98/200] Iter[938/938]\t\tLoss: 0.3638 Acc@1: 92.030%\n",
            "| Validation Epoch #98\t\t\tLoss: 0.0020 Acc@1: 95.73%\n",
            "| Elapsed time : 1:12:07\n",
            "| Epoch [ 99/200] Iter[938/938]\t\tLoss: 0.6200 Acc@1: 92.018%\n",
            "| Validation Epoch #99\t\t\tLoss: 0.0042 Acc@1: 95.64%\n",
            "| Elapsed time : 1:12:52\n",
            "| Epoch [100/200] Iter[938/938]\t\tLoss: 0.2390 Acc@1: 91.883%\n",
            "| Validation Epoch #100\t\t\tLoss: 0.0031 Acc@1: 95.54%\n",
            "| Elapsed time : 1:13:36\n",
            "| Epoch [101/200] Iter[938/938]\t\tLoss: 0.3367 Acc@1: 91.925%\n",
            "| Validation Epoch #101\t\t\tLoss: 0.0028 Acc@1: 95.52%\n",
            "| Elapsed time : 1:14:21\n",
            "| Epoch [102/200] Iter[938/938]\t\tLoss: 0.1002 Acc@1: 91.883%\n",
            "| Validation Epoch #102\t\t\tLoss: 0.0103 Acc@1: 95.50%\n",
            "| Elapsed time : 1:15:05\n",
            "| Epoch [103/200] Iter[938/938]\t\tLoss: 0.1589 Acc@1: 92.088%\n",
            "| Validation Epoch #103\t\t\tLoss: 0.0049 Acc@1: 95.72%\n",
            "| Elapsed time : 1:15:49\n",
            "| Epoch [104/200] Iter[938/938]\t\tLoss: 0.2827 Acc@1: 91.970%\n",
            "| Validation Epoch #104\t\t\tLoss: 0.0050 Acc@1: 95.58%\n",
            "| Elapsed time : 1:16:32\n",
            "| Epoch [105/200] Iter[938/938]\t\tLoss: 0.0631 Acc@1: 92.122%\n",
            "| Validation Epoch #105\t\t\tLoss: 0.0055 Acc@1: 95.77%\n",
            "| Elapsed time : 1:17:16\n",
            "| Epoch [106/200] Iter[938/938]\t\tLoss: 0.2215 Acc@1: 92.042%\n",
            "| Validation Epoch #106\t\t\tLoss: 0.0050 Acc@1: 95.71%\n",
            "| Elapsed time : 1:18:02\n",
            "| Epoch [107/200] Iter[938/938]\t\tLoss: 0.1440 Acc@1: 91.952%\n",
            "| Validation Epoch #107\t\t\tLoss: 0.0049 Acc@1: 95.63%\n",
            "| Elapsed time : 1:18:45\n",
            "| Epoch [108/200] Iter[938/938]\t\tLoss: 0.1857 Acc@1: 91.985%\n",
            "| Validation Epoch #108\t\t\tLoss: 0.0026 Acc@1: 95.64%\n",
            "| Elapsed time : 1:19:29\n",
            "| Epoch [109/200] Iter[938/938]\t\tLoss: 0.1395 Acc@1: 91.903%\n",
            "| Validation Epoch #109\t\t\tLoss: 0.0037 Acc@1: 95.58%\n",
            "| Elapsed time : 1:20:12\n",
            "| Epoch [110/200] Iter[938/938]\t\tLoss: 0.1669 Acc@1: 92.095%\n",
            "| Validation Epoch #110\t\t\tLoss: 0.0108 Acc@1: 95.51%\n",
            "| Elapsed time : 1:20:56\n",
            "| Epoch [111/200] Iter[938/938]\t\tLoss: 0.2853 Acc@1: 92.063%\n",
            "| Validation Epoch #111\t\t\tLoss: 0.0084 Acc@1: 95.44%\n",
            "| Elapsed time : 1:21:42\n",
            "| Epoch [112/200] Iter[938/938]\t\tLoss: 0.1670 Acc@1: 91.958%\n",
            "| Validation Epoch #112\t\t\tLoss: 0.0083 Acc@1: 95.59%\n",
            "| Elapsed time : 1:22:28\n",
            "| Epoch [113/200] Iter[938/938]\t\tLoss: 0.1941 Acc@1: 92.193%\n",
            "| Validation Epoch #113\t\t\tLoss: 0.0083 Acc@1: 95.78%\n",
            "| Elapsed time : 1:23:13\n",
            "| Epoch [114/200] Iter[938/938]\t\tLoss: 0.1908 Acc@1: 92.197%\n",
            "| Validation Epoch #114\t\t\tLoss: 0.0143 Acc@1: 95.71%\n",
            "| Elapsed time : 1:23:58\n",
            "| Epoch [115/200] Iter[938/938]\t\tLoss: 0.1927 Acc@1: 92.022%\n",
            "| Validation Epoch #115\t\t\tLoss: 0.0141 Acc@1: 95.76%\n",
            "| Elapsed time : 1:24:44\n",
            "| Epoch [116/200] Iter[938/938]\t\tLoss: 0.3116 Acc@1: 91.977%\n",
            "| Validation Epoch #116\t\t\tLoss: 0.0074 Acc@1: 95.69%\n",
            "| Elapsed time : 1:25:29\n",
            "| Epoch [117/200] Iter[938/938]\t\tLoss: 0.3468 Acc@1: 92.132%\n",
            "| Validation Epoch #117\t\t\tLoss: 0.0100 Acc@1: 95.79%\n",
            "| Elapsed time : 1:26:14\n",
            "| Epoch [118/200] Iter[938/938]\t\tLoss: 0.1286 Acc@1: 92.142%\n",
            "| Validation Epoch #118\t\t\tLoss: 0.0061 Acc@1: 95.78%\n",
            "| Elapsed time : 1:26:59\n",
            "| Epoch [119/200] Iter[938/938]\t\tLoss: 0.2601 Acc@1: 92.188%\n",
            "| Validation Epoch #119\t\t\tLoss: 0.0080 Acc@1: 95.71%\n",
            "| Elapsed time : 1:27:43\n",
            "| Epoch [120/200] Iter[938/938]\t\tLoss: 0.2774 Acc@1: 92.077%\n",
            "| Validation Epoch #120\t\t\tLoss: 0.0104 Acc@1: 95.53%\n",
            "| Elapsed time : 1:28:27\n",
            "| Epoch [121/200] Iter[938/938]\t\tLoss: 0.3813 Acc@1: 91.953%\n",
            "| Validation Epoch #121\t\t\tLoss: 0.0072 Acc@1: 95.61%\n",
            "| Elapsed time : 1:29:11\n",
            "| Epoch [122/200] Iter[938/938]\t\tLoss: 0.1493 Acc@1: 92.010%\n",
            "| Validation Epoch #122\t\t\tLoss: 0.0087 Acc@1: 95.66%\n",
            "| Elapsed time : 1:29:56\n",
            "| Epoch [123/200] Iter[938/938]\t\tLoss: 0.1355 Acc@1: 92.070%\n",
            "| Validation Epoch #123\t\t\tLoss: 0.0042 Acc@1: 95.86%\n",
            "| Elapsed time : 1:30:40\n",
            "| Epoch [124/200] Iter[938/938]\t\tLoss: 0.3401 Acc@1: 91.963%\n",
            "| Validation Epoch #124\t\t\tLoss: 0.0049 Acc@1: 95.62%\n",
            "| Elapsed time : 1:31:24\n",
            "| Epoch [125/200] Iter[938/938]\t\tLoss: 0.1123 Acc@1: 92.163%\n",
            "| Validation Epoch #125\t\t\tLoss: 0.0055 Acc@1: 95.72%\n",
            "| Elapsed time : 1:32:08\n",
            "| Epoch [126/200] Iter[938/938]\t\tLoss: 0.6421 Acc@1: 92.062%\n",
            "| Validation Epoch #126\t\t\tLoss: 0.0059 Acc@1: 95.70%\n",
            "| Elapsed time : 1:32:52\n",
            "| Epoch [127/200] Iter[938/938]\t\tLoss: 0.1716 Acc@1: 92.013%\n",
            "| Validation Epoch #127\t\t\tLoss: 0.0148 Acc@1: 95.66%\n",
            "| Elapsed time : 1:33:37\n",
            "| Epoch [128/200] Iter[938/938]\t\tLoss: 0.3922 Acc@1: 92.047%\n",
            "| Validation Epoch #128\t\t\tLoss: 0.0098 Acc@1: 95.81%\n",
            "| Elapsed time : 1:34:21\n",
            "| Epoch [129/200] Iter[938/938]\t\tLoss: 0.1852 Acc@1: 91.950%\n",
            "| Validation Epoch #129\t\t\tLoss: 0.0155 Acc@1: 95.70%\n",
            "| Elapsed time : 1:35:05\n",
            "| Epoch [130/200] Iter[938/938]\t\tLoss: 0.2339 Acc@1: 92.192%\n",
            "| Validation Epoch #130\t\t\tLoss: 0.0090 Acc@1: 95.62%\n",
            "| Elapsed time : 1:35:49\n",
            "| Epoch [131/200] Iter[938/938]\t\tLoss: 0.5516 Acc@1: 92.043%\n",
            "| Validation Epoch #131\t\t\tLoss: 0.0057 Acc@1: 95.80%\n",
            "| Elapsed time : 1:36:34\n",
            "| Epoch [132/200] Iter[938/938]\t\tLoss: 0.1881 Acc@1: 92.188%\n",
            "| Validation Epoch #132\t\t\tLoss: 0.0079 Acc@1: 95.65%\n",
            "| Elapsed time : 1:37:19\n",
            "| Epoch [133/200] Iter[938/938]\t\tLoss: 0.1473 Acc@1: 92.170%\n",
            "| Validation Epoch #133\t\t\tLoss: 0.0154 Acc@1: 95.69%\n",
            "| Elapsed time : 1:38:05\n",
            "| Epoch [134/200] Iter[938/938]\t\tLoss: 0.0771 Acc@1: 92.175%\n",
            "| Validation Epoch #134\t\t\tLoss: 0.0104 Acc@1: 95.74%\n",
            "| Elapsed time : 1:38:51\n",
            "| Epoch [135/200] Iter[938/938]\t\tLoss: 0.0686 Acc@1: 92.068%\n",
            "| Validation Epoch #135\t\t\tLoss: 0.0068 Acc@1: 95.72%\n",
            "| Elapsed time : 1:39:36\n",
            "| Epoch [136/200] Iter[938/938]\t\tLoss: 0.1100 Acc@1: 92.092%\n",
            "| Validation Epoch #136\t\t\tLoss: 0.0051 Acc@1: 95.83%\n",
            "| Elapsed time : 1:40:22\n",
            "| Epoch [137/200] Iter[938/938]\t\tLoss: 0.2662 Acc@1: 92.163%\n",
            "| Validation Epoch #137\t\t\tLoss: 0.0064 Acc@1: 95.82%\n",
            "| Elapsed time : 1:41:07\n",
            "| Epoch [138/200] Iter[938/938]\t\tLoss: 0.2894 Acc@1: 92.107%\n",
            "| Validation Epoch #138\t\t\tLoss: 0.0052 Acc@1: 95.82%\n",
            "| Elapsed time : 1:41:53\n",
            "| Epoch [139/200] Iter[938/938]\t\tLoss: 0.3675 Acc@1: 92.310%\n",
            "| Validation Epoch #139\t\t\tLoss: 0.0039 Acc@1: 95.78%\n",
            "| Elapsed time : 1:42:39\n",
            "| Epoch [140/200] Iter[938/938]\t\tLoss: 0.3406 Acc@1: 92.350%\n",
            "| Validation Epoch #140\t\t\tLoss: 0.0047 Acc@1: 95.77%\n",
            "| Elapsed time : 1:43:25\n",
            "| Epoch [141/200] Iter[938/938]\t\tLoss: 0.1268 Acc@1: 92.242%\n",
            "| Validation Epoch #141\t\t\tLoss: 0.0038 Acc@1: 95.95%\n",
            "| Elapsed time : 1:44:11\n",
            "| Epoch [142/200] Iter[938/938]\t\tLoss: 0.4728 Acc@1: 92.087%\n",
            "| Validation Epoch #142\t\t\tLoss: 0.0039 Acc@1: 95.70%\n",
            "| Elapsed time : 1:44:57\n",
            "| Epoch [143/200] Iter[938/938]\t\tLoss: 0.3728 Acc@1: 92.080%\n",
            "| Validation Epoch #143\t\t\tLoss: 0.0072 Acc@1: 95.88%\n",
            "| Elapsed time : 1:45:44\n",
            "| Epoch [144/200] Iter[938/938]\t\tLoss: 0.2138 Acc@1: 92.100%\n",
            "| Validation Epoch #144\t\t\tLoss: 0.0026 Acc@1: 95.74%\n",
            "| Elapsed time : 1:46:31\n",
            "| Epoch [145/200] Iter[938/938]\t\tLoss: 0.2434 Acc@1: 92.110%\n",
            "| Validation Epoch #145\t\t\tLoss: 0.0064 Acc@1: 95.78%\n",
            "| Elapsed time : 1:47:17\n",
            "| Epoch [146/200] Iter[938/938]\t\tLoss: 0.0891 Acc@1: 92.253%\n",
            "| Validation Epoch #146\t\t\tLoss: 0.0049 Acc@1: 95.72%\n",
            "| Elapsed time : 1:48:02\n",
            "| Epoch [147/200] Iter[938/938]\t\tLoss: 0.1146 Acc@1: 91.950%\n",
            "| Validation Epoch #147\t\t\tLoss: 0.0081 Acc@1: 95.98%\n",
            "| Elapsed time : 1:48:48\n",
            "| Epoch [148/200] Iter[938/938]\t\tLoss: 0.3263 Acc@1: 92.133%\n",
            "| Validation Epoch #148\t\t\tLoss: 0.0045 Acc@1: 95.76%\n",
            "| Elapsed time : 1:49:37\n",
            "| Epoch [149/200] Iter[938/938]\t\tLoss: 0.2719 Acc@1: 92.087%\n",
            "| Validation Epoch #149\t\t\tLoss: 0.0041 Acc@1: 95.82%\n",
            "| Elapsed time : 1:50:24\n",
            "| Epoch [150/200] Iter[938/938]\t\tLoss: 0.3656 Acc@1: 92.238%\n",
            "| Validation Epoch #150\t\t\tLoss: 0.0037 Acc@1: 95.92%\n",
            "| Elapsed time : 1:51:09\n",
            "| Epoch [151/200] Iter[938/938]\t\tLoss: 0.3249 Acc@1: 92.313%\n",
            "| Validation Epoch #151\t\t\tLoss: 0.0049 Acc@1: 95.60%\n",
            "| Elapsed time : 1:51:55\n",
            "| Epoch [152/200] Iter[938/938]\t\tLoss: 0.1871 Acc@1: 92.267%\n",
            "| Validation Epoch #152\t\t\tLoss: 0.0051 Acc@1: 95.62%\n",
            "| Elapsed time : 1:52:41\n",
            "| Epoch [153/200] Iter[938/938]\t\tLoss: 0.0957 Acc@1: 92.158%\n",
            "| Validation Epoch #153\t\t\tLoss: 0.0059 Acc@1: 95.68%\n",
            "| Elapsed time : 1:53:27\n",
            "| Epoch [154/200] Iter[938/938]\t\tLoss: 0.2143 Acc@1: 92.232%\n",
            "| Validation Epoch #154\t\t\tLoss: 0.0101 Acc@1: 95.91%\n",
            "| Elapsed time : 1:54:14\n",
            "| Epoch [155/200] Iter[938/938]\t\tLoss: 0.1759 Acc@1: 92.272%\n",
            "| Validation Epoch #155\t\t\tLoss: 0.0115 Acc@1: 95.91%\n",
            "| Elapsed time : 1:55:00\n",
            "| Epoch [156/200] Iter[938/938]\t\tLoss: 0.3847 Acc@1: 92.123%\n",
            "| Validation Epoch #156\t\t\tLoss: 0.0132 Acc@1: 95.87%\n",
            "| Elapsed time : 1:55:46\n",
            "| Epoch [157/200] Iter[938/938]\t\tLoss: 0.1313 Acc@1: 92.415%\n",
            "| Validation Epoch #157\t\t\tLoss: 0.0030 Acc@1: 95.65%\n",
            "| Elapsed time : 1:56:33\n",
            "| Epoch [158/200] Iter[938/938]\t\tLoss: 0.1478 Acc@1: 92.237%\n",
            "| Validation Epoch #158\t\t\tLoss: 0.0111 Acc@1: 95.92%\n",
            "| Elapsed time : 1:57:20\n",
            "| Epoch [159/200] Iter[938/938]\t\tLoss: 0.0855 Acc@1: 91.990%\n",
            "| Validation Epoch #159\t\t\tLoss: 0.0092 Acc@1: 95.76%\n",
            "| Elapsed time : 1:58:08\n",
            "| Epoch [160/200] Iter[938/938]\t\tLoss: 0.3406 Acc@1: 92.175%\n",
            "| Validation Epoch #160\t\t\tLoss: 0.0107 Acc@1: 95.85%\n",
            "| Elapsed time : 1:58:55\n",
            "| Epoch [161/200] Iter[938/938]\t\tLoss: 0.3124 Acc@1: 92.293%\n",
            "| Validation Epoch #161\t\t\tLoss: 0.0077 Acc@1: 95.78%\n",
            "| Elapsed time : 1:59:43\n",
            "| Epoch [162/200] Iter[938/938]\t\tLoss: 0.2261 Acc@1: 92.080%\n",
            "| Validation Epoch #162\t\t\tLoss: 0.0206 Acc@1: 95.55%\n",
            "| Elapsed time : 2:00:30\n",
            "| Epoch [163/200] Iter[938/938]\t\tLoss: 0.1875 Acc@1: 92.167%\n",
            "| Validation Epoch #163\t\t\tLoss: 0.0125 Acc@1: 95.80%\n",
            "| Elapsed time : 2:01:17\n",
            "| Epoch [164/200] Iter[938/938]\t\tLoss: 0.1917 Acc@1: 92.425%\n",
            "| Validation Epoch #164\t\t\tLoss: 0.0073 Acc@1: 95.82%\n",
            "| Elapsed time : 2:02:03\n",
            "| Epoch [165/200] Iter[938/938]\t\tLoss: 0.1988 Acc@1: 92.378%\n",
            "| Validation Epoch #165\t\t\tLoss: 0.0047 Acc@1: 95.98%\n",
            "| Elapsed time : 2:02:51\n",
            "| Epoch [166/200] Iter[938/938]\t\tLoss: 0.2758 Acc@1: 92.293%\n",
            "| Validation Epoch #166\t\t\tLoss: 0.0050 Acc@1: 95.76%\n",
            "| Elapsed time : 2:03:38\n",
            "| Epoch [167/200] Iter[938/938]\t\tLoss: 0.4109 Acc@1: 92.285%\n",
            "| Validation Epoch #167\t\t\tLoss: 0.0060 Acc@1: 95.86%\n",
            "| Elapsed time : 2:04:24\n",
            "| Epoch [168/200] Iter[938/938]\t\tLoss: 0.5687 Acc@1: 92.390%\n",
            "| Validation Epoch #168\t\t\tLoss: 0.0042 Acc@1: 95.65%\n",
            "| Elapsed time : 2:05:11\n",
            "| Epoch [169/200] Iter[938/938]\t\tLoss: 0.2261 Acc@1: 92.238%\n",
            "| Validation Epoch #169\t\t\tLoss: 0.0036 Acc@1: 95.89%\n",
            "| Elapsed time : 2:05:58\n",
            "| Epoch [170/200] Iter[938/938]\t\tLoss: 0.1787 Acc@1: 92.380%\n",
            "| Validation Epoch #170\t\t\tLoss: 0.0033 Acc@1: 95.66%\n",
            "| Elapsed time : 2:06:44\n",
            "| Epoch [171/200] Iter[938/938]\t\tLoss: 0.3267 Acc@1: 92.217%\n",
            "| Validation Epoch #171\t\t\tLoss: 0.0050 Acc@1: 95.89%\n",
            "| Elapsed time : 2:07:32\n",
            "| Epoch [172/200] Iter[938/938]\t\tLoss: 0.3734 Acc@1: 92.218%\n",
            "| Validation Epoch #172\t\t\tLoss: 0.0042 Acc@1: 95.67%\n",
            "| Elapsed time : 2:08:19\n",
            "| Epoch [173/200] Iter[938/938]\t\tLoss: 0.1950 Acc@1: 92.343%\n",
            "| Validation Epoch #173\t\t\tLoss: 0.0074 Acc@1: 95.83%\n",
            "| Elapsed time : 2:09:06\n",
            "| Epoch [174/200] Iter[938/938]\t\tLoss: 0.4299 Acc@1: 92.270%\n",
            "| Validation Epoch #174\t\t\tLoss: 0.0039 Acc@1: 95.72%\n",
            "| Elapsed time : 2:09:53\n",
            "| Epoch [175/200] Iter[938/938]\t\tLoss: 0.1528 Acc@1: 92.243%\n",
            "| Validation Epoch #175\t\t\tLoss: 0.0033 Acc@1: 95.82%\n",
            "| Elapsed time : 2:10:40\n",
            "| Epoch [176/200] Iter[938/938]\t\tLoss: 0.1308 Acc@1: 92.300%\n",
            "| Validation Epoch #176\t\t\tLoss: 0.0038 Acc@1: 95.89%\n",
            "| Elapsed time : 2:11:27\n",
            "| Epoch [177/200] Iter[938/938]\t\tLoss: 0.1415 Acc@1: 92.328%\n",
            "| Validation Epoch #177\t\t\tLoss: 0.0051 Acc@1: 96.02%\n",
            "| Elapsed time : 2:12:14\n",
            "| Epoch [178/200] Iter[938/938]\t\tLoss: 0.5180 Acc@1: 92.250%\n",
            "| Validation Epoch #178\t\t\tLoss: 0.0082 Acc@1: 95.85%\n",
            "| Elapsed time : 2:13:02\n",
            "| Epoch [179/200] Iter[938/938]\t\tLoss: 0.4092 Acc@1: 92.300%\n",
            "| Validation Epoch #179\t\t\tLoss: 0.0055 Acc@1: 95.87%\n",
            "| Elapsed time : 2:13:49\n",
            "| Epoch [180/200] Iter[938/938]\t\tLoss: 0.6362 Acc@1: 92.387%\n",
            "| Validation Epoch #180\t\t\tLoss: 0.0056 Acc@1: 95.70%\n",
            "| Elapsed time : 2:14:36\n",
            "| Epoch [181/200] Iter[938/938]\t\tLoss: 0.2474 Acc@1: 92.350%\n",
            "| Validation Epoch #181\t\t\tLoss: 0.0036 Acc@1: 95.78%\n",
            "| Elapsed time : 2:15:23\n",
            "| Epoch [182/200] Iter[938/938]\t\tLoss: 0.4894 Acc@1: 92.225%\n",
            "| Validation Epoch #182\t\t\tLoss: 0.0033 Acc@1: 96.01%\n",
            "| Elapsed time : 2:16:11\n",
            "| Epoch [183/200] Iter[938/938]\t\tLoss: 0.1588 Acc@1: 92.440%\n",
            "| Validation Epoch #183\t\t\tLoss: 0.0086 Acc@1: 95.89%\n",
            "| Elapsed time : 2:16:58\n",
            "| Epoch [184/200] Iter[938/938]\t\tLoss: 0.3080 Acc@1: 92.297%\n",
            "| Validation Epoch #184\t\t\tLoss: 0.0097 Acc@1: 95.70%\n",
            "| Elapsed time : 2:17:45\n",
            "| Epoch [185/200] Iter[938/938]\t\tLoss: 0.2643 Acc@1: 92.230%\n",
            "| Validation Epoch #185\t\t\tLoss: 0.0080 Acc@1: 95.89%\n",
            "| Elapsed time : 2:18:32\n",
            "| Epoch [186/200] Iter[938/938]\t\tLoss: 0.2434 Acc@1: 92.400%\n",
            "| Validation Epoch #186\t\t\tLoss: 0.0045 Acc@1: 95.67%\n",
            "| Elapsed time : 2:19:20\n",
            "| Epoch [187/200] Iter[938/938]\t\tLoss: 0.3199 Acc@1: 92.255%\n",
            "| Validation Epoch #187\t\t\tLoss: 0.0046 Acc@1: 95.87%\n",
            "| Elapsed time : 2:20:07\n",
            "| Epoch [188/200] Iter[938/938]\t\tLoss: 0.0556 Acc@1: 92.177%\n",
            "| Validation Epoch #188\t\t\tLoss: 0.0045 Acc@1: 95.76%\n",
            "| Elapsed time : 2:20:53\n",
            "| Epoch [189/200] Iter[938/938]\t\tLoss: 0.1248 Acc@1: 92.382%\n",
            "| Validation Epoch #189\t\t\tLoss: 0.0126 Acc@1: 95.83%\n",
            "| Elapsed time : 2:21:40\n",
            "| Epoch [190/200] Iter[938/938]\t\tLoss: 0.3975 Acc@1: 92.255%\n",
            "| Validation Epoch #190\t\t\tLoss: 0.0033 Acc@1: 95.81%\n",
            "| Elapsed time : 2:22:26\n",
            "| Epoch [191/200] Iter[938/938]\t\tLoss: 0.3219 Acc@1: 92.328%\n",
            "| Validation Epoch #191\t\t\tLoss: 0.0054 Acc@1: 95.97%\n",
            "| Elapsed time : 2:23:13\n",
            "| Epoch [192/200] Iter[938/938]\t\tLoss: 0.1180 Acc@1: 92.423%\n",
            "| Validation Epoch #192\t\t\tLoss: 0.0031 Acc@1: 95.86%\n",
            "| Elapsed time : 2:24:01\n",
            "| Epoch [193/200] Iter[938/938]\t\tLoss: 0.1845 Acc@1: 92.415%\n",
            "| Validation Epoch #193\t\t\tLoss: 0.0019 Acc@1: 95.80%\n",
            "| Elapsed time : 2:24:48\n",
            "| Epoch [194/200] Iter[938/938]\t\tLoss: 0.5825 Acc@1: 92.205%\n",
            "| Validation Epoch #194\t\t\tLoss: 0.0044 Acc@1: 95.96%\n",
            "| Elapsed time : 2:25:35\n",
            "| Epoch [195/200] Iter[938/938]\t\tLoss: 0.1934 Acc@1: 92.198%\n",
            "| Validation Epoch #195\t\t\tLoss: 0.0043 Acc@1: 95.96%\n",
            "| Elapsed time : 2:26:22\n",
            "| Epoch [196/200] Iter[938/938]\t\tLoss: 0.4033 Acc@1: 92.363%\n",
            "| Validation Epoch #196\t\t\tLoss: 0.0045 Acc@1: 96.12%\n",
            "| Elapsed time : 2:27:09\n",
            "| Epoch [197/200] Iter[938/938]\t\tLoss: 0.2533 Acc@1: 92.378%\n",
            "| Validation Epoch #197\t\t\tLoss: 0.0040 Acc@1: 95.91%\n",
            "| Elapsed time : 2:27:58\n",
            "| Epoch [198/200] Iter[938/938]\t\tLoss: 0.3045 Acc@1: 92.318%\n",
            "| Validation Epoch #198\t\t\tLoss: 0.0023 Acc@1: 95.86%\n",
            "| Elapsed time : 2:28:45\n",
            "| Epoch [199/200] Iter[938/938]\t\tLoss: 0.3366 Acc@1: 92.390%\n",
            "| Validation Epoch #199\t\t\tLoss: 0.0066 Acc@1: 95.99%\n",
            "| Elapsed time : 2:29:32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REH5NkZ6R1Hx"
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxpeC5mFV9N7"
      },
      "source": [
        "#args\n",
        "\n",
        "def compute_forgetting_statistics(diag_stats, npresentations):\n",
        "\n",
        "    presentations_needed_to_learn = {}\n",
        "    unlearned_per_presentation = {}\n",
        "    margins_per_presentation = {}\n",
        "    first_learned = {}\n",
        "\n",
        "    for example_id, example_stats in diag_stats.items():\n",
        "\n",
        "        # Skip 'train' and 'test' keys of diag_stats\n",
        "        if not isinstance(example_id, str):\n",
        "\n",
        "            # Forgetting event is a transition in accuracy from 1 to 0\n",
        "            presentation_acc = np.array(example_stats[1][:npresentations])\n",
        "            transitions = presentation_acc[1:] - presentation_acc[:-1]\n",
        "\n",
        "            # Find all presentations when forgetting occurs\n",
        "            if len(np.where(transitions == -1)[0]) > 0:\n",
        "                unlearned_per_presentation[example_id] = np.where(\n",
        "                    transitions == -1)[0] + 2\n",
        "            else:\n",
        "                unlearned_per_presentation[example_id] = []\n",
        "\n",
        "            # Find number of presentations needed to learn example, \n",
        "            # e.g. last presentation when acc is 0\n",
        "            if len(np.where(presentation_acc == 0)[0]) > 0:\n",
        "                presentations_needed_to_learn[example_id] = np.where(\n",
        "                    presentation_acc == 0)[0][-1] + 1\n",
        "            else:\n",
        "                presentations_needed_to_learn[example_id] = 0\n",
        "\n",
        "            # Find the misclassication margin for each presentation of the example\n",
        "            margins_per_presentation = np.array(\n",
        "                example_stats[2][:npresentations])\n",
        "\n",
        "            # Find the presentation at which the example was first learned, \n",
        "            # e.g. first presentation when acc is 1\n",
        "            if len(np.where(presentation_acc == 1)[0]) > 0:\n",
        "                first_learned[example_id] = np.where(\n",
        "                    presentation_acc == 1)[0][0]\n",
        "            else:\n",
        "                first_learned[example_id] = np.nan\n",
        "\n",
        "    return presentations_needed_to_learn, unlearned_per_presentation, margins_per_presentation, first_learned\n",
        "\n",
        "\n",
        "# Sorts examples by number of forgetting counts during training, in ascending order\n",
        "# If an example was never learned, it is assigned the maximum number of forgetting counts\n",
        "# If multiple training runs used, sort examples by the sum of their forgetting counts over all runs\n",
        "#\n",
        "# unlearned_per_presentation_all: list of dictionaries, one per training run\n",
        "# first_learned_all: list of dictionaries, one per training run\n",
        "# npresentations: number of training epochs\n",
        "#\n",
        "# Returns 2 numpy arrays containing the sorted example ids and corresponding forgetting counts\n",
        "#\n",
        "def sort_examples_by_forgetting(unlearned_per_presentation_all,\n",
        "                                first_learned_all, npresentations):\n",
        "\n",
        "    # Initialize lists\n",
        "    example_original_order = []\n",
        "    example_stats = []\n",
        "\n",
        "    for example_id in unlearned_per_presentation_all[0].keys():\n",
        "\n",
        "        # Add current example to lists\n",
        "        example_original_order.append(example_id)\n",
        "        example_stats.append(0)\n",
        "\n",
        "        # Iterate over all training runs to calculate the total forgetting count for current example\n",
        "        for i in range(len(unlearned_per_presentation_all)):\n",
        "\n",
        "            # Get all presentations when current example was forgotten during current training run\n",
        "            stats = unlearned_per_presentation_all[i][example_id]\n",
        "\n",
        "            # If example was never learned during current training run, add max forgetting counts\n",
        "            if np.isnan(first_learned_all[i][example_id]):\n",
        "                example_stats[-1] += npresentations\n",
        "            else:\n",
        "                example_stats[-1] += len(stats)\n",
        "\n",
        "    print('Number of unforgettable examples: {}'.format(\n",
        "        len(np.where(np.array(example_stats) == 0)[0])))\n",
        "    return np.array(example_original_order)[np.argsort(\n",
        "        example_stats)], np.sort(example_stats)\n",
        "\n",
        "\n",
        "# Checks whether a given file name matches a list of specified arguments\n",
        "#\n",
        "# fname: string containing file name\n",
        "# args_list: list of strings containing argument names and values, i.e. [arg1, val1, arg2, val2,..]\n",
        "#\n",
        "# Returns 1 if filename matches the filter specified by the argument list, 0 otherwise\n",
        "#\n",
        "def check_filename(fname, args_list):\n",
        "\n",
        "    # # If no arguments are specified to filter by, pass filename\n",
        "    # if args_list is None:\n",
        "    #     return 1\n",
        "\n",
        "    # for arg_ind in list(args_list):#np.arange(0, len(args_list), 2):\n",
        "    #     arg = str(arg_ind)\n",
        "    #     arg_value = str(args_list[arg_ind])\n",
        "\n",
        "    #     # Check if filename matches the current arg and arg value\n",
        "    #     if arg + '_' + arg_value + '__' not in fname:\n",
        "    #         print('skipping file: ' + fname)\n",
        "    #         return 0\n",
        "\n",
        "    return 1\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-1mebn3WLiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f85b195-929d-4d89-81df-1e211c97040f"
      },
      "source": [
        "\n",
        "\n",
        "args = {'output_dir': 'permuted_mnist_results',\n",
        "        'output_name': 'permuted_mnist_sorted',\n",
        "        'input_dir':'permuted_mnist_results',\n",
        "        'epochs': 20,\n",
        "        'input_fname_args':\n",
        "        {\n",
        "            'dataset': 'permuted_mnist',\n",
        "            'no_droput': False,\n",
        "            'sorting_file': 'none',\n",
        "            'remove_n': 0,\n",
        "            'keep_lowest_n': 0\n",
        "\n",
        "        }\n",
        "\n",
        "        }\n",
        "\n",
        "# Initialize lists to collect forgetting stastics per example across multiple training runs\n",
        "unlearned_per_presentation_all, first_learned_all = [], []\n",
        "\n",
        "for d, _, fs in os.walk(args['input_dir']):\n",
        "    for f in fs:\n",
        "\n",
        "        # Find the files that match input_fname_args and compute forgetting statistics\n",
        "        if f.endswith('stats_dict.pkl') and check_filename(\n",
        "                f, args['input_fname_args']):\n",
        "            print('including file: ' + f)\n",
        "\n",
        "            # Load the dictionary compiled during training run\n",
        "            with open(os.path.join(d, f), 'rb') as fin:\n",
        "                loaded = pickle.load(fin)\n",
        "\n",
        "            # Compute the forgetting statistics per example for training run\n",
        "            _, unlearned_per_presentation, _, first_learned = compute_forgetting_statistics(\n",
        "                loaded, args['epochs'])\n",
        "\n",
        "            unlearned_per_presentation_all.append(\n",
        "                unlearned_per_presentation)\n",
        "            first_learned_all.append(first_learned)\n",
        "\n",
        "if len(unlearned_per_presentation_all) == 0:\n",
        "    print('No input files found in {} that match {}'.format(\n",
        "        args['input_dir'], args['input_fname_args']))\n",
        "else:\n",
        "\n",
        "    # Sort examples by forgetting counts in ascending order, over one or more training runs\n",
        "    ordered_examples, ordered_values = sort_examples_by_forgetting(\n",
        "        unlearned_per_presentation_all, first_learned_all, args['epochs'])\n",
        "\n",
        "    # Save sorted output\n",
        "    if args['output_name'].endswith('.pkl'):\n",
        "        with open(os.path.join(args['output_dir'], args['output_name']),\n",
        "                  'wb') as fout:\n",
        "            pickle.dump({\n",
        "                'indices': ordered_examples,\n",
        "                'forgetting counts': ordered_values\n",
        "            }, fout)\n",
        "    else:\n",
        "        with open(\n",
        "                os.path.join(args['output_dir'], args['output_name'] + '.pkl'),\n",
        "                'wb') as fout:\n",
        "            pickle.dump({\n",
        "                'indices': ordered_examples,\n",
        "                'forgetting counts': ordered_values\n",
        "            }, fout)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "including file: 0__stats_dict.pkl\n",
            "including file: 1__stats_dict.pkl\n",
            "including file: 2__stats_dict.pkl\n",
            "including file: 3__stats_dict.pkl\n",
            "including file: 4__stats_dict.pkl\n",
            "including file: 5__stats_dict.pkl\n",
            "including file: 6__stats_dict.pkl\n",
            "including file: 7__stats_dict.pkl\n",
            "including file: 8__stats_dict.pkl\n",
            "including file: 9__stats_dict.pkl\n",
            "including file: 10__stats_dict.pkl\n",
            "including file: 11__stats_dict.pkl\n",
            "including file: 12__stats_dict.pkl\n",
            "including file: 13__stats_dict.pkl\n",
            "including file: 14__stats_dict.pkl\n",
            "including file: 15__stats_dict.pkl\n",
            "including file: 16__stats_dict.pkl\n",
            "including file: 17__stats_dict.pkl\n",
            "including file: 18__stats_dict.pkl\n",
            "including file: 19__stats_dict.pkl\n",
            "including file: 20__stats_dict.pkl\n",
            "including file: 21__stats_dict.pkl\n",
            "including file: 22__stats_dict.pkl\n",
            "including file: 23__stats_dict.pkl\n",
            "including file: 24__stats_dict.pkl\n",
            "including file: 25__stats_dict.pkl\n",
            "including file: 26__stats_dict.pkl\n",
            "including file: 27__stats_dict.pkl\n",
            "including file: 28__stats_dict.pkl\n",
            "including file: 29__stats_dict.pkl\n",
            "including file: 30__stats_dict.pkl\n",
            "including file: 31__stats_dict.pkl\n",
            "including file: 32__stats_dict.pkl\n",
            "including file: 33__stats_dict.pkl\n",
            "including file: 34__stats_dict.pkl\n",
            "including file: 35__stats_dict.pkl\n",
            "including file: 36__stats_dict.pkl\n",
            "including file: 37__stats_dict.pkl\n",
            "including file: 38__stats_dict.pkl\n",
            "including file: 39__stats_dict.pkl\n",
            "including file: 40__stats_dict.pkl\n",
            "including file: 41__stats_dict.pkl\n",
            "including file: 42__stats_dict.pkl\n",
            "including file: 43__stats_dict.pkl\n",
            "including file: 44__stats_dict.pkl\n",
            "including file: 45__stats_dict.pkl\n",
            "including file: 46__stats_dict.pkl\n",
            "including file: 47__stats_dict.pkl\n",
            "including file: 48__stats_dict.pkl\n",
            "including file: 49__stats_dict.pkl\n",
            "including file: 50__stats_dict.pkl\n",
            "including file: 51__stats_dict.pkl\n",
            "including file: 52__stats_dict.pkl\n",
            "including file: 53__stats_dict.pkl\n",
            "including file: 54__stats_dict.pkl\n",
            "including file: 55__stats_dict.pkl\n",
            "including file: 56__stats_dict.pkl\n",
            "including file: 57__stats_dict.pkl\n",
            "including file: 58__stats_dict.pkl\n",
            "including file: 59__stats_dict.pkl\n",
            "including file: 60__stats_dict.pkl\n",
            "including file: 61__stats_dict.pkl\n",
            "including file: 62__stats_dict.pkl\n",
            "including file: 63__stats_dict.pkl\n",
            "including file: 64__stats_dict.pkl\n",
            "including file: 65__stats_dict.pkl\n",
            "including file: 66__stats_dict.pkl\n",
            "including file: 67__stats_dict.pkl\n",
            "including file: 68__stats_dict.pkl\n",
            "including file: 69__stats_dict.pkl\n",
            "including file: 70__stats_dict.pkl\n",
            "including file: 71__stats_dict.pkl\n",
            "including file: 72__stats_dict.pkl\n",
            "including file: 73__stats_dict.pkl\n",
            "including file: 74__stats_dict.pkl\n",
            "including file: 75__stats_dict.pkl\n",
            "including file: 76__stats_dict.pkl\n",
            "including file: 77__stats_dict.pkl\n",
            "including file: 78__stats_dict.pkl\n",
            "including file: 79__stats_dict.pkl\n",
            "including file: 80__stats_dict.pkl\n",
            "including file: 81__stats_dict.pkl\n",
            "including file: 82__stats_dict.pkl\n",
            "including file: 83__stats_dict.pkl\n",
            "including file: 84__stats_dict.pkl\n",
            "including file: 85__stats_dict.pkl\n",
            "including file: 86__stats_dict.pkl\n",
            "including file: 87__stats_dict.pkl\n",
            "including file: 88__stats_dict.pkl\n",
            "including file: 89__stats_dict.pkl\n",
            "including file: 90__stats_dict.pkl\n",
            "including file: 91__stats_dict.pkl\n",
            "including file: 92__stats_dict.pkl\n",
            "including file: 93__stats_dict.pkl\n",
            "including file: 94__stats_dict.pkl\n",
            "including file: 95__stats_dict.pkl\n",
            "including file: 96__stats_dict.pkl\n",
            "including file: 97__stats_dict.pkl\n",
            "including file: 98__stats_dict.pkl\n",
            "including file: 99__stats_dict.pkl\n",
            "including file: 100__stats_dict.pkl\n",
            "including file: 101__stats_dict.pkl\n",
            "including file: 102__stats_dict.pkl\n",
            "including file: 103__stats_dict.pkl\n",
            "including file: 104__stats_dict.pkl\n",
            "including file: 105__stats_dict.pkl\n",
            "including file: 106__stats_dict.pkl\n",
            "including file: 107__stats_dict.pkl\n",
            "including file: 108__stats_dict.pkl\n",
            "including file: 109__stats_dict.pkl\n",
            "including file: 110__stats_dict.pkl\n",
            "including file: 111__stats_dict.pkl\n",
            "including file: 112__stats_dict.pkl\n",
            "including file: 113__stats_dict.pkl\n",
            "including file: 114__stats_dict.pkl\n",
            "including file: 115__stats_dict.pkl\n",
            "including file: 116__stats_dict.pkl\n",
            "including file: 117__stats_dict.pkl\n",
            "including file: 118__stats_dict.pkl\n",
            "including file: 119__stats_dict.pkl\n",
            "including file: 120__stats_dict.pkl\n",
            "including file: 121__stats_dict.pkl\n",
            "including file: 122__stats_dict.pkl\n",
            "including file: 123__stats_dict.pkl\n",
            "including file: 124__stats_dict.pkl\n",
            "including file: 125__stats_dict.pkl\n",
            "including file: 126__stats_dict.pkl\n",
            "including file: 127__stats_dict.pkl\n",
            "including file: 128__stats_dict.pkl\n",
            "including file: 129__stats_dict.pkl\n",
            "including file: 130__stats_dict.pkl\n",
            "including file: 131__stats_dict.pkl\n",
            "including file: 132__stats_dict.pkl\n",
            "including file: 133__stats_dict.pkl\n",
            "including file: 134__stats_dict.pkl\n",
            "including file: 135__stats_dict.pkl\n",
            "including file: 136__stats_dict.pkl\n",
            "including file: 137__stats_dict.pkl\n",
            "including file: 138__stats_dict.pkl\n",
            "including file: 139__stats_dict.pkl\n",
            "including file: 140__stats_dict.pkl\n",
            "including file: 141__stats_dict.pkl\n",
            "including file: 142__stats_dict.pkl\n",
            "including file: 143__stats_dict.pkl\n",
            "including file: 144__stats_dict.pkl\n",
            "including file: 145__stats_dict.pkl\n",
            "including file: 146__stats_dict.pkl\n",
            "including file: 147__stats_dict.pkl\n",
            "including file: 148__stats_dict.pkl\n",
            "including file: 149__stats_dict.pkl\n",
            "including file: 150__stats_dict.pkl\n",
            "including file: 151__stats_dict.pkl\n",
            "including file: 152__stats_dict.pkl\n",
            "including file: 153__stats_dict.pkl\n",
            "including file: 154__stats_dict.pkl\n",
            "including file: 155__stats_dict.pkl\n",
            "including file: 156__stats_dict.pkl\n",
            "including file: 157__stats_dict.pkl\n",
            "including file: 158__stats_dict.pkl\n",
            "including file: 159__stats_dict.pkl\n",
            "including file: 160__stats_dict.pkl\n",
            "including file: 161__stats_dict.pkl\n",
            "including file: 162__stats_dict.pkl\n",
            "including file: 163__stats_dict.pkl\n",
            "including file: 164__stats_dict.pkl\n",
            "including file: 165__stats_dict.pkl\n",
            "including file: 166__stats_dict.pkl\n",
            "including file: 167__stats_dict.pkl\n",
            "including file: 168__stats_dict.pkl\n",
            "including file: 169__stats_dict.pkl\n",
            "including file: 170__stats_dict.pkl\n",
            "including file: 171__stats_dict.pkl\n",
            "including file: 172__stats_dict.pkl\n",
            "including file: 173__stats_dict.pkl\n",
            "including file: 174__stats_dict.pkl\n",
            "including file: 175__stats_dict.pkl\n",
            "including file: 176__stats_dict.pkl\n",
            "including file: 177__stats_dict.pkl\n",
            "including file: 178__stats_dict.pkl\n",
            "including file: 179__stats_dict.pkl\n",
            "including file: 180__stats_dict.pkl\n",
            "including file: 181__stats_dict.pkl\n",
            "including file: 182__stats_dict.pkl\n",
            "including file: 183__stats_dict.pkl\n",
            "including file: 184__stats_dict.pkl\n",
            "including file: 185__stats_dict.pkl\n",
            "including file: 186__stats_dict.pkl\n",
            "including file: 187__stats_dict.pkl\n",
            "including file: 188__stats_dict.pkl\n",
            "including file: 189__stats_dict.pkl\n",
            "including file: 190__stats_dict.pkl\n",
            "including file: 191__stats_dict.pkl\n",
            "including file: 192__stats_dict.pkl\n",
            "including file: 193__stats_dict.pkl\n",
            "including file: 194__stats_dict.pkl\n",
            "including file: 195__stats_dict.pkl\n",
            "including file: 196__stats_dict.pkl\n",
            "including file: 197__stats_dict.pkl\n",
            "including file: 198__stats_dict.pkl\n",
            "including file: 199__stats_dict.pkl\n",
            "Number of unforgettable examples: 16849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmot3w3l48SN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}